%!TEX root = ../Main.tex
\section{Processes}
\label{s:Processes}

Each combinator defines a stream process which is expressed in terms of a simple imperative program with a local heap. The process pulls data from an aribrary number if input streams and pushes data to at least one output stream. \ben{Mention how we're going to convert arrays to streams, but we don't need to give the details yet}. 


% -----------------------------------------------------------------------------
\subsection{Grouping}
\begin{figure}
\begin{alltt}
 group 
   = \(\lambda\) (s1: Stream a) (s2: Stream a). 
     \(\nu\) (first: Bool)  (last: a) (value: a) (L0..L3: Label).
\end{alltt}
\begin{code}
     process
     { ins:    { s1 }
     , outs:   { s2 }
     , heap:   { first = True, last = 0, value = 0 }
     , label:  L0
     , instrs: { L0 = pull s1 value                    L1 {}
               , L1 = case (first || (last /= value))  L2 {}  L3 {}
               , L2 = push s2 value                    L3 { last = value, first = False }
               , L3 = drop s1                          L0 {} } }
\end{code}
\caption{The group combinator}
\label{fig:Process:Group}
\end{figure}


The definition of the @group@ combinator which removes consecutive elements from its input stream is given in Figure~\ref{fig:Process:Group}. The @group@ combinator has two parameters, @s1@ and @s2@ which bind the input and output streams respectively. The \emph{nu-binders} like \mbox{$\nu$ @(first: Bool)@...} indicate that each time we instantiate the @group@ combinator we should create fresh names for @first@, @last@ and so on that do not conflict with other instantiations. 

The body of the combinator is a record that defines the process. The @ins@ field of the record defines the set of input streams and the @outs@ field the set of output streams. The @heap@ field gives the starting state of each of the local variables. The @instrs@ field contains a set of labeled instructions that define the program, while the @label@ field gives the label of the first instruction. 

The instruction @(pull s1 value L1 {})@ pulls the next element from stream @s1@, writes it into the heap variable @value@, then continues with the instruction at label @L1@. The set @{}@ at the end of the instruction can be used to update values in the heap, but as we do not need to perform heap updates in this instance we leave it empty. 

The instruction @(case (first || (last /= value)) L2 {} L3 {})@ checks whether the predicate @(first || last /= value)@ is true and if so continues with the instruction at label @L2@, otherwise continues with the instruction at label @L3@. 

The instruction @(push s2 value L3 { last = value, first = False })@ pushes the value @value@ to the stream @s2@ and continues with the instruction at label @L3@, once the heap has been updated to set variable @last@ to @value@ and @first@ to @False@. 

Finally, the instruction @(drop s1 L0 {})@ signals that the current element that we pulled from stream @s1@ is no longer required, and contines with the instruction at @L0@. This @drop@ instruction is a synchronisation primitive that is used in the fusion algorithm but has no direct operational meaning. We discuss @drop@ further in \TODO{ref}. 

Overall, the @first@ variable tracks whether we are dealing with the first value from the stream, @last@ holds the last value pulled from the stream (or 0 if none have been read yet), and @value@ holds the current value pulled from the stream. The process emits the first value pulled from the stream and every value that is different from the last one that was pulled. For example, when executed on the input stream $[1, 2, 2, 3]$, this process will produce the output $[1, 2, 3]$.

Note that we refer to @group@ itself as a \emph{combinator} as it is parameterised by the names of the streams it deals with (@s1@ and @s2@). Once the combinator has been instantiated to work on particular streams the result is a concrete \emph{process}. 


% Recall that @group@ removes consecutive duplicates from its input stream.
% It has one input, @file1@, and one output, @uniques@.

% There are three variables in the heap: @first@ is initialised to @True@ as the first pulled value has no last value to compare against; @value@ stores the most recently pulled value, and @last@ stores the last pulled value.

% The @last@ and @value@ variables are initialised to $0$, but could be initialised to any value: these initial values will not be used.
% The initial label is @L0@, which pulls from the input @file1@. This blocks waiting for an input value, and when one is received, it is stored in @value@ and the process moves to @L1@.
% Instruction @L1@ performs a case analysis on a boolean: if it is the first read value, or the last value is not equal to the most recent value, it jumps to @L2@; otherwise it jumps to @L3@.
% Instruction @L2@ pushes the most recent value to the output and jumps to @L3@, updating the last value to the most recent value, and setting first to @False@.
% Finally, the instruction for @L3@ `drops' the recently pulled value from @file1@ and jumps back to @L0@.
% This dropping is required to coordinate between multiple processes that read from the same input: now that the read value from @file1@ has been dropped, another process is free to pull the next value from @file1@ if it so wishes.



% -----------------------------------------------------------------------------
\subsection{Merging}
\begin{figure}
\begin{alltt}
 merge
   = \(\lambda\) (s1: Stream Nat) (s2: Stream Nat) (s3: Stream Nat). 
     \(\nu\) (x1: Nat)  (x2: Nat) (L0..L3: Label) (FIXME).
\end{alltt}
\begin{code}
     process
     { ins:    { s1, s2 }
     , outs:   { s3 }
     , heap:   { x1 = 0, x2 = 0 }
     , label:  L0
     , instrs: { L0 = pull s1 x1     L1 {}
               , L1 = pull s2 x2     C0 {}
               , C0 = case (x1 < x2) A0 {}  B0 {}
               , A0 = push s3 x1     A1 {}
               , A1 = drop s1        A2 {}
               , A2 = pull s1 x1     C0 {}
               , B0 = push s3 x2     B1 {}
               , B1 = drop s2        B2 {}
               , B2 = pull s2  x2    C0 {} } }
\end{code}
\caption{The merge combinator}
\label{fig:Process:Merge}
\end{figure}

The definition of the @merge@ combinator which merges two input streams is given in Figure~\ref{fig:Process:Merge}. The combinator binds the two input streams to @s1@ and @s2@, while the output stream is @s3@. The two heap variables @x1@ and @x2@ are used to store the currently read values from each input. The forms of the instructions used to define this combinator are the same as the @group@ combinator in the previous section. We only need the four basic @pull@, @case@, @push@ and @drop@ instructions.

As this process merges infinite streams, if we execute it using a finite prefix then the final state will be an intermediate one that may not yet have pushed all available output. For example, if we execute the process with the input streams $[1, 4]$ and $[2, 3, 100]$ then the values $[1, 2, 3, 4]$ will be pushed to the output. The machine will arrive at instruction @L?@ which blocks waiting for the next value to be pulled from @arr1@. We discuss how to handle finite streams later in ~\S\ref{s:Finite}


% The instructions at @L0@ and @L1@ initialise the process by reading the first values from each stream, then move to @C0@.
% Instruction @C0@ checks which value is smaller: if the value @a@ read from @file1@ is smaller, it moves to @A0@; otherwise it moves to @B0@.
% Instructions @A0@, @A1@ and @A2@ push @file1@'s value to the output, drop it, and pull another one before moving back to @C0@ to compare again.
% Instructions @B0@, @B1@ and @B2@ are equivalent, except performing on @file2@ instead.


% -----------------------------------------------------------------------------
\subsection{Fusion}
\includegraphics[scale=0.8]{figures/state-group.pdf}


We are now ready to take our @group@ and @merge@ processes and fuse them together. 
We create a new process with inputs @file1@ and @file2@ and outputs @uniques@ and @merged@.
In order to make the fused process easier to understand, we have performed some minor optimisations on the output of the fusion algorithm described in~\S\ref{s:Fusion}, such as removing @jump@s and merging the two variables @value@ and @a@ into a single variable @value_a@.
Rather than explaining the process in detail, we simply note that the fused process is an interleaved scheduling of the two, starting with executing @group@, then executing @merge@, and going back to @group@ whenever pulling from @file1@ again.
The labels @L0@-@L2@ and @A1@-@A4@ are much the same as those in @group@, while @C0@, @A0@-@A2@ and @B0@-@B2@ are similar to those in @merge@.

\begin{code}
process (group/merge)
     ins: file1 file2
    outs: uniques merged
    heap: {first = True, last, value_a, b}
   label: L0
  blocks: L0 = pull file1   value_a               L1
          L1 = case (or first (last /= value_a))  L2    L3
          L2 = push uniques value_a               L3{last = value_a, first = False}
          L3 = pull file2 b                       C0

          C0 = case (value_a < b)                 A0    B0

          A0 = push merged  value_a               A1
          A1 = drop file1                         A2
          A2 = pull file1   value_a               A3
          A3 = case (or first (last /= value_a))  A4    C0
          A4 = push uniques value_a               C0{last = value_a, first=False}

          B0 = push merged b                      B1
          B1 = drop file2                         B2
          B2 = pull file2  b                      C0
\end{code}


\clearpage{}
\subsection{Process definitions}

%% This figure is referenced below, in 'Process definitions', but putting it up here before the big fused process stops the fused one from splitting across multiple pages.
\input{figures/ProcessDef.tex}

The grammar for processes is shown in figure~\ref{fig:Process:Def}.
Channels, labels and variables are specified by some external, globally unique set of names.
For values and expressions we use a simple untyped lambda calculus with a few primitives chosen to facilitate the examples.

\amos{Settle on a terminology for channels / streams. Are both necessary?}
Streams are the abstract data flowing through while channels are particular endpoints, so a stream can have multiple channels.
A stream can have at most one output channel, and any number of input channels.

A process defines a stream computation, taking any number of input streams and producing at least one output stream.
Processes have an optional name in parentheses used only for descriptive purposes, and does not need to be unique.
The input streams are paired with an input state which is used for coordinating multiple processes during evaluation; in process definitions before any evaluation has occurred this should be @none@.
The input states are explained in detail later in \S\ref{s:Process:Eval}.

The output streams are in some sense ``owned'' by the process that produces them.
While a stream may be consumed by any number of processes, each stream can only appear as the output for one process.
This ensures a sort of determinism in the scheduling of multiple processes; if different processes could push to the same stream, the order of values would depend on the scheduled order.
A process may, however, produce multiple output streams.

The heap is used for evaluation of expressions.
Each process has its own private heap, meaning that the only communication between processes occurs by streams.

The label is the current block, and each block is the instruction to be executed in the current state.
Instructions can pull from a stream, drop an already pulled value, push a value, perform an if/case analysis on a boolean, or perform an internal jump.

As usual with Kahn processes~\cite{kahn1976coroutines}, pulling from a channel is blocking.
Unlike normal Kahn processes however, pushing to a channel can also block: each consumer has a single-element buffer and pushing only succeeds when all buffers are empty.
After values have been pulled, they must be disposed of with @drop@: this empties the value from the buffer and allows the producer process to push to that stream.

All instructions take as an argument the next label state to jump to, which includes any variable updates that should be performed on the private heap at the same time.
Combining variable update with stream instructions simplifies the fusion process in~\S\ref{s:Fusion}, as some parts of fusion need to perform both at once.


A process network is a set of multiple processes that can be evaluated concurrently.
The intersection of all process outputs should be empty - there should be no overlap.
Any inputs that are not mentioned as outputs of processes are assumed to be external inputs - their values will be provided by the environment.
Processes form the essence of stream computation, and a single process can be given a straightforward sequential semantics by mapping to an imperative language.
By fusing multiple processes into a single one, we are effectively giving a sequential interpretation for concurrent processes.


% \subsection{Map/map}
% \label{s:Process:MapMap}
% 
% One of the simplest combinators is @map@.
% This might need to go elsewhere.
% As well as needing a better example than map/map.
% Let's start with the process definition for @map@.
% The inputs here is actually a map with @as=none@, but we leave the value off when it is @none@.
% The next labels for the instructions are also shortened as @map0@ instead of writing the empty heap update afterwards.
% Mention that the initial heap has the names but no values, but they could be initialised to whatever.
% It doesn't matter since they'll be written before anything is read.
% 
% \begin{code}
% map f = process (map f)
%      ins: as
%     outs: bs
%     heap: {a = 0}
%    label: map0
%   blocks: map0 = pull as    a  map1
%           map1 = push bs (f a) map2
%           map2 = drop as       map0
% \end{code}
% 
% As well as a ``combinator network'', a function comprised exclusively of process combinators.
% The input streams are supplied as arguments, and output streams as return values.
% \begin{code}
% mapMap f g xs
%  = let ys = map f xs
%        zs = map g ys
%    in  zs
% \end{code}
% 
% It is not hard to assume that, given the process definitions and a combinator network, we can produce a process network.
% This is simple enough for a paragraph prose description.
% It's just a bit of inlining and renaming everything to be unique.
% 
% \begin{code}
% process (map f)
%      ins: xs
%     outs: ys
%     heap: {x}
%    label: p0
%   blocks: p0 = pull xs    x  p1
%           p1 = push ys (f x) p2
%           p2 = drop xs       p0
% process (map g)
%      ins: ys
%     outs: zs
%     heap: {y}
%    label: q0
%   blocks: q0 = pull ys    y  q1
%           q1 = push zs (g y) q2
%           q2 = drop ys       q0
% \end{code}
% 
% Now we can perform some kind of fusion on this network, resulting in one process that computes, as output, both @ys@ and @zs@.
% Later, when producing imperative code for this, the output pushes to @ys@ can be ignored and changed to jumps, as the original combinator network did not return them.
% 
% \begin{code}
% process (map f / map g)
%      ins: xs
%     outs: ys zs
%     heap: {x, y, _ys}
%    label: p0q0
%   blocks: p0q0            = pull xs    x  p1q0
%           p1q0            = push ys (f x) p2q0-pending-ys { _ys = f x }
%           p2q0-pending-ys = drop xs       p0q0-pending-ys
%           p0q0-pending-ys = jump          p0q1-have-ys    { y = _ys }
%           p0q1-have-ys    = push zs (g y) p0q2-have-ys
%           p0q2-have-ys    = jump          p0q0
% \end{code}

\subsection{Evaluation}
\label{s:Process:Eval}

\input{figures/ProcessEval.tex}
\input{figures/ProcessFeed.tex}

We now describe evaluation of processes and process networks.
We split evaluation into three main parts:
\begin{itemize}
\item Injection, where values are inserted into a process' input buffer.
Injection is only possible when the process input buffer is empty.
\item Shaking, where a process takes a step from one label to another.
Shaking a process results in a new process as well as an output message.
If a process pushes to a stream, the push value must be able to be injected to other processes.
\item Feeding, where an environment of input values are fed to the processes, and output values are collected.
This is the `top level' of evaluation that uses both injection and shaking.
\end{itemize}

Evaluation of a process network is non-deterministic, in that at any point there are many possible processes that can take a step.
However, because each process itself is deterministic and has blocking reads, overall evaluation is deterministic as per Kahn process networks.
That is: the order in which values are pushed to different output streams is not deterministic, but the order and values for a particular output stream \emph{are} deterministic.

Note that while process network evaluation is non-deterministic and concurrent, evaluating a single process is sequential and deterministic: code generation for fused processes only needs to deal with the sequential case.

Rules in figure~\ref{fig:Process:Eval:Inject} are about injecting values into a process; these are the values used when the process performs a @pull@.
The injected values may be pushed values from other processes for internal streams, or may come from an external source for the overall network's input streams.
Injection is just about orchestrating values between processes, and no actual computation happens here; it just makes values available to be pulled.

Injection can only happen when a process is ready to receive more input.
A process has a single element buffer for each input, stored in its input state.
This can be either @none@ meaning an empty buffer, @pending@ meaning a single value has been added to the buffer but has not been read yet, or @have@ meaning the value was added and in the process of being used.

(InjectValue) allows a value to be injected only when the input state is @none@, meaning the buffer is empty.
An attempt to inject a value while the buffer is @pending@ or @have@ would require an unbounded (or at least multiple element) buffer.
Injecting the value puts the value as @pending@ in the buffer.

(InjectIgnore) allows processes that do not use a particular input stream to ignore an injected input.

(ProcessesInject) performs injection over a process network.
Every process in the network must have the value injected into it.
This means if multiple processes read from that stream, all input buffers for that stream must be empty.

The rules in figure~\ref{fig:Process:Eval:Shake} are the `shake' part of evaluation, where actual computation occurs. 
As usual, $\alpha$ denotes the message type, with $\tau$ being an internal message. The \Push~ message is a single value being output on a channel.


The judgment form for shaking a single instruction $\ProcBlockShake{b}{i}{\Sigma}{\alpha}{l'}{i'}{u'}$
executes an instruction $b$ with the input states $i$ and the heap $\Sigma$.
The output message $\alpha$ can be an internal state change or an emitted value.
The result also has the new label, the new input state buffers, and the substitution to apply to the heap.

The two judgment forms for shaking processes are $\ProcShake{p}{\alpha}{p'}$ and $\ProcsShake{\sgl{p}}{\alpha}{\sgl{p}}$.
The process shaking just shakes a single instruction and updates the process.
Shaking a process network chooses a single process to shake, then if the result is an emitted value, that value is injected into all the other processes in the network.

(Pull) takes an already injected value from the input buffer, which changes its state from @pending@ to @have@.
The result substitution sets the variable to the pulled value, as well as any substitutions in the \Next~ of the instruction.

(Drop) changes the input buffer state from @have@ to @none@. A drop can only be executed after pull.

(Push) evaluates the push expression $e$ under the heap, and sends the value as the message.

(Jump) simply returns the new label and substitution.

(CaseT) and (CaseF) evaluate the case expression $e$ and jump to the true or false label depending on the value.

(Shake) unwraps a single process and evaluates the instruction.
The instruction updates are evaluated and updated in the process heap.
It updates the process with the new label, input state and heap.

(ProcessesInternal) chooses one process from the network and evaluates it.
When the process evaluates with an internal message ($\tau$), the entire network evaluates by replacing that process.

(ProcessesPush) chooses one process from the network and evaluates it, where the process evaluates with a push message.
The emitted push message is then injected into all other processes in the network, which means they must either ignore the channel or be ready to add it to their buffer.
If the process tries to emit a push message but it cannot be injected into all other processes, the push fails and another process will be tried.
The entire network then emits the same message.

The rules in figure~\ref{fig:Process:Eval:Feed} are the `feed' part of evaluation, where external input values are fed into a process network and output values are accumulated.
The judgment form for feeding is $\ProcsFeed{\ti{inputs}}{\ti{network}}{\ti{streams}}{\ti{network}'}$.
The input map $\ti{inputs}$ contains values for the network inputs: network outputs are not allowed, but ignored channels can have values.
The result $\ti{streams}$ contains the original inputs as well as accumulated output values.
Feeding evaluates the process network until all input values have been injected.

% Note that the result stream and network are not canonical, as an infinite @push@ loop has an infinite number of evaluations.
% The feed form does not ensure that the processes themselves have finished evaluating, only that all input values have been injected.

(FeedStart) is the axiom form where all input values have been injected and there are no input values left.
This is the start of evaluation.

(FeedInternal) first recursively feeds its input accumulator and process network, then allows the resulting network to take an internal step.
The internal step does not affect the accumulators.
% The recursive closure is performed \emph{before} the internal step rather than after for proof engineering reasons: it allows an extra step to be added to the end of a feed evaluation relatively easily.

(FeedPush) works similarly to (FeedInternal) except that the process network emits a push message.
The pushed value is added to the end of the accumulator for that channel.

(FeedExternal) allows inputs to be injected into the process network.
For any channel $c$ which is not an output of one of the processes, we take the last value off its list.
The recursive feed is evaluated with the last value removed from the accumulators.
The last value is then injected into the network, and added back to the result accumulators.



% -- cuts ---------------------------------------------------------------------
% BL: I don't think describing iota works at this point. This combinator is not used in the motivating example, so skipping to it seems disjointed.

% Before describing the @group@ process, we start by looking at one of the simplest combinators, @iota@, which produces a stream of increasing numbers.
% It takes no inputs, and produces one output stream @xs@.
% Each process has its own local heap where the values are stored, and in this case we initialise the local variable @i@ to @0@.
% This variable will be incremented and pushed.
% Each process also has a current label, which denotes the instruction to perform next.
% The initial label for @iota@ is @L0@.
% Each process has a mapping from labels to instructions.
% In this case we have two instructions, @L0@ and @L1@.
% The instruction for @L0@ pushes the current value of variable @i@ onto the output stream, then proceeds to move %to label @L1@. Instruction @L1@ moves back to @L0@, while also incrementing the variable @i@.

% \begin{code}
% process (iota)
%      ins: 
%     outs: xs
%     heap: {i = 0}
%    label: L0
%   blocks: L0 = push xs i  L1
%           L1 = jump       L0{i = i + 1}
% \end{code}

% When executed, this program produces an infinite stream of increasing numbers: $0, 1, 2\ldots$ while the label alternates between @L0@ and @L1@.

% The @uniques = group file1@ is a more interesting example.

