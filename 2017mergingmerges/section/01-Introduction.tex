%!TEX root = ../Main.tex
\section{Introduction}
\label{s:Introduction}

% Fusion for array programs is important for removing intermediate arrays, reducing memory traffic and reducing allocations.
% However, when dealing with data too large to fit in memory such as tables on disk, removing intermediate arrays becomes essential rather than just desirable.
% Attempting to create an intermediate array of such amounts of data would lead to thrashing and swapping to disk, or perhaps even running out of swap.
% For these situations, some sort of assurance of total fusion is required: either the program can be fused with no intermediate arrays or unbounded buffers, or it will not compile at all.

Suppose we have two large files containing identifiers, and wish to perform some analysis on them.
Both files are sorted, but may contain duplicates.
We wish to retrieve the unique set from the first file, as well as the union of both files.
Can we perform both of these tasks at once, without reading the input files multiple times?
\amos{More compelling justification. Why would someone want to do this?}
\amos{Should this example also have readFiles?}
\begin{code}
uniquesUnion file1 file2
 = do   let uniques = group file1
        let merged  = merge file1 file2
        let union   = group merged
        writeFile "uniques.txt" uniques
        writeFile "union.txt"   unions
\end{code}

% This example is somewhat incorrect because the streams here are pull streams,
% but the fused result cannot return pull streams (since we don't know what order they will be pulled)
% \begin{code}
% uniquesUnion (file1 file2 : Stream Id) : (Stream Id, Stream Id)
%  = let uniques = group file1
%        merged  = merge file1 file2
%        union   = group merged
%    in (uniques, union)
% \end{code}

In this example the @group@ combinator filters out consecutive duplicates, while @merge@ concatenates two sorted inputs such that the output remains sorted.
There are a few interesting things about this example.
First, the access pattern of @merge@ is \emph{value-dependent}: the order in which @merge@ reads values from @file1@ or @file2@ depends on the values in the files.
If all the values in @file1@ are smaller than the values in @file2@, then @merge@ will pull values from @file1@ before pulling from @file2@, and vice versa.
Second, note that @file1@ is used twice.
If we wish to ensure that each value is only read from the file once, we must coordinate between the two use sites: when @uniques@ requires a new value it must ensure that @union@ is ready to receive a new value, and vice versa.
Note that we cannot just execute @uniques@ while storing the read values in a buffer, as this may require more memory than is available.

In order to fuse this example, we require both pull \emph{and} push streams.
The input streams must be pull streams since the order values are required is determined by the @merge@ combinator.
For the same reason, the outputs sent to each @writeFile@ must be push streams.


Sadly, stream fusion~\cite{coutts2007stream} and other forms of shortcut fusion~\cite{jones2001playing} cannot fuse this example because there are multiple consumers.
Synchronous dataflow languages~\cite{mandel2010lucy} and data flow inspired fusion~\cite{lippmeier2013data} cannot fuse this because they do not support value-dependent input patterns, and furthermore data flow fusion only supports a restricted set of baked-in combinators.
Polarised data flow fusion~\cite{lippmeier2016polarized} is able to fuse this case, but requires the program to be rewritten with explicitly push/pull annotations on each stream.

Our approach is to treat each combinator as a separate process with communication channels between them.
Each process is a very restricted sequential imperative program with commands such as @pull@ for reading from an input channel and @push@ for writing an output.
Multiple processes form a network where processes are executed concurrently.
Fusion takes a concurrent network and \emph{sequentialises} it into a single process by choosing a particular evaluation order that requires no buffering.

The example above has three combinators, so the process network has three processes.
The two @writeFile@s outputs are treated as sinks that values can be pushed to at any time, and are not converted to processes.
During code generation, any output values from the @uniques@ and @union@ streams are sent to the corresponding @writeFile@ sink, but we do not address code generation in this paper.

The process for @uniques@ is defined by the @group@ combinator, and can be thought of as an imperative loop: first it reads from its input stream @file1@ and stores that in a local variable.
It also keeps track of the last pulled value, and compares that against the newly read value.
If they are different, it pushes the new value to its output stream @uniques@.
In either case, it updates the last pulled value and loops back to the start to pull from @file1@ again.

The process for @merged@ is defined by the @merge@ combinator, which starts by reading from both @file1@ and @file2@ and storing these in local variables.
It then compares its two values to see which is the smaller.
If the value from @file1@ is smaller, it pushes that value and pulls a new value from @file1@, otherwise it pushes the value from @file2@ and pulls from @file2@.
This is performed in a loop.

We fuse these two processes by interleaving the two such that the shared input @file1@ is only pulled from when both processes agree.
The new process pulls from @file1@, which is copied to the variables for both processes.
The @uniques@ process now has all it needs to execute, so it checks the value against the last pulled value, pushes if necessary, and goes back to try to pull from @file1@ again.
At this stage the @merged@ process still has a value from @file1@ that it has pulled but not used, so @uniques@ cannot pull from @file1@ again.
We now let @merged@ run, pulling from @file2@ and checking which is smaller.
If the value from @file1@ is smaller, the value is emitted and @merged@ wishes to pull a new value from @file1@.
Both processes now agree on pulling from @file1@ again, so the new value is pulled and @uniques@ can run again.
Otherwise if the value from @file1@ is not smaller, the value from @file2@ is emitted and @merged@ pulls from @file2@ with no coordination required.


We make the following contributions:
\begin{itemize}
\item a process calculus for encoding infinite streaming programs (\S\ref{s:Processes});
\item an algorithm for fusing processes, the first which supports splits, joins and arbitrary combinators (\S\ref{s:Fusion});
\item a formalisation and proof of soundness in Coq (\S\ref{s:Proofs});
\item an informal description of the required extensions to support finite streams (\S\ref{s:Finite});
\item and show our processes are general enough for many combinators, including segmented operations (\S\ref{s:Combinators}).
\end{itemize}


