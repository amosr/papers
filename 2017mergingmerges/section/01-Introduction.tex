%!TEX root = ../Main.tex
\section{Introduction}
\label{s:Introduction}

Suppose we have two input streams of numeric identifiers, and wish to perform some analysis on these identifiers. The identifiers from both streams arrive sorted, but may include duplicates. We wish to produce an output stream of unique identifiers from the first input stream, as well as produce the unique union of identifiers from both streams. Here is how we might write the source code, where @S@ is for @S@-tream.
\begin{code}
  uniquesUnion : S Nat -> S Nat -> (S Nat, S Nat)
  uniquesUnion sIn1 sIn2
   = let  sUnique = group sIn1
          sMerged = merge sIn1 sIn2
          sUnion  = group sMerged
     in   (sUnique, sUnion)
\end{code}

In this implementation the @group@ operator filters out consecutive duplicates, while @merge@ combines two sorted streams so that the output remains sorted. This example has a few interesting properties. Firstly, the data-access pattern of @merge@ is \emph{value-dependent}, meaning that the order in which this operator pulls values from @sIn1@ and @sIn2@ depends on the values themselves. If all the values from @sIn1@ are smaller than the values in @sIn2@, then @merge@ will pull all values from @sIn1@ before pulling the rest from @sIn2@, and vice versa. Secondly, although @sIn1@ occurs twice in the program, at runtime we only want to handle the elements of each stream once. To achieve this, the compiled program must coordinate between the two uses of @sIn1@, so that a new value is read only when both the @group@ and @merge@ operators are ready to receive it. Finally, as the stream length is assumed to be unbounded, we cannot buffer an arbitrary number of elements read from either stream, or risk running out of local storage space.

We might try to implement each of the operators as a separate concurrent process, and send each identifier value using an intra-process communication mechanism. Developing such an implementation could be easy or hard, depending on what language features are available for concurrency. However, worrying about the \emph{performance tuning} of such a system, such as whether we need back-pressure to prevent buffers from being overrun, or how to chunk the stream data to reduce the amount of communication overhead, is invariably a headache. 

We might instead define some sort of uniform sequential interface for data sources, with a single `pull' function that provides the next value in each stream. Each operator could be given this interface, so that the next value from each result stream is computed on demand. This is approach is commonly taken with implementations of physical operators in database systems. However, the `pull only' model does not support operators with multiple outputs, such as our derived @uniquesUnion@ operator, at least not without unbounded buffering. Suppose a consumer pulls many elements from the @sUnique@ output stream. In order to perform the @group@ operation, the implementation needs to pull the corresponding source elements from the @sIn1@ input stream \emph{as well} as buffering a arbitrary number of them. It needs to buffer these elements because they are also needed to perform the @merge@ operation. When a consumer finally pulls from @sUnion@ we will be able to drain the buffer of @sIn1@ elements, but not before.

Instead, for a single threaded program, we want to perform \emph{stream fusion}, which takes the implied dataflow network and produces a simple sequential loop that gets the job done without requiring extra process-control abstractions and without requiring unbounded buffering. Sadly, existing stream fusion transformations cannot handle our example. As observed by \citet{kay2009you}, both pull-based and push-based fusion have fundamental limitations. Pull-based systems such as short-cut stream fusion~\cite{coutts2007stream} cannot handle cases where a particular stream or intermediate result is used by multiple consumers. We refer to this situation as a \mbox{\emph{split} --- in the} dataflow network of our example the flow from input stream @sIn1@ is split into both the @group@ and @merge@ consumers. 

Push-based systems such as foldr/build fusion~\cite{gill1993short} also cannot fuse our example either, because they do not support operators with multiple inputs. We refer to such a situation as a \emph{join} --- in our example the @merge@ operator expresses a join in the data-flow network. Some systems support both pull and push: data flow inspired array fusion using series expressions~\cite{lippmeier2013data} allows both splits and joins but only for a limited, predefined set of operators. More recent work on polarized data flow fusion~\cite{lippmeier2016polarized} \emph{is} able to fuse our example, but requires the program to be rewritten to use explicitly polarized stream types. 

Synchronous dataflow languages such as Lucy-n~\cite{mandel2010lucy} reject value-dependent operators with value dependent control flow such as @merge@, while general dataflow languages fall back on less performant dynamic scheduling for these cases \cite{bouakaz2013real}. The polyhedral array fusion model~\cite{feautrier2011polyhedron} is used for loop transformations in imperative programs, but operates at a much lower level. The polyhedral model is based around affine loops, which makes it difficult to support filter-like operators such as @group@ and @merge@.

In this paper we present Machine Fusion, a new approach. Each operator is expressed as a restricted, sequential imperative program with commands that include both @pull@ for reading from an input stream and @push@ for writing to an output stream. We view each operator as a process in a concurrent process network, and represent the control flow of the process as a simple state machine. Our fusion transform then \emph{sequentializes} the concurrent data flow network into a single process, by choosing a particular interleaving of the operator code that requires no unbounded intermediate buffers. When the fusion transform succeeds we know it has worked. There is no need to inspect intermediate representations of the compiler to debug poor performance, which is a common problem in systems based on general purpose program transformations \cite{lippmeier2012:guiding}.

In summary, we make the following contributions:
\begin{itemize}
\item a process calculus for encoding infinite streaming programs (\S\ref{s:Processes});
\item an algorithm for fusing these processes, the first to support splits and joins (\S\ref{s:Fusion});
\item numerical results that demonstrate that the algorithm is well behaved when the number of fused processes is large. The size of the fused result program is not excessive. (\S\ref{s:Evaluation}).
\item a formalization and proof of soundness for the core fusion algorithm in Coq (\S\ref{s:Proofs});
\end{itemize}

Our fusion transform for infinite stream programs could also serve as the basis for an \emph{array} fusion system, using a natural extension to finite streams. We discuss this extension in \S\ref{s:Finite}.

