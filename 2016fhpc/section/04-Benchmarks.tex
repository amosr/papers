%!TEX root = ../Main.tex
\section{Benchmarks}
\label{s:Benchmarks}

We are currently using Icicle in production, over medium-sized datasets (around three hundred gigabytes) that fit on a single disk.
While future versions will require incremental computation, the runtime processing medium-sized datasets has been small enough to run over the entire dataset each time.
These initial results have been very promising, and we are currently implementing distribution across multiple machines for datasets that do not fit on a single disk.
Incremental computation becomes more important for this distributed case, as it can potentially avoid copying terabytes of data over the network.

We evaluated Icicle by replacing an R script which ran weekly in production.
This R script works over around three hundred gigabytes of PSV data.
It computes twelve queries over each of the thirty-one input tables, computing 372 queries in total.

The R script for this takes fifteen hours to run and is 3,566 lines, 2,311 of which are code.
In contrast, our Icicle queries take seven minutes to run, and the dictionary describing the queries is 191 lines of code.

It is also important to note that, as a very constrained streaming language, memory usage of Icicle queries is more or less constant in the input size, and runtime is linear.
This is very important, as new data is received regularly and the input size grows.
This does not appear to be the case with the R script, which continually requires larger computers with more memory, in order to finish in a reasonable time.
One of the benefits of Icicle is allowing data scientists to focus on queries, without having to worry about performance.

\begin{figure}

\begin{tikzpicture}
\begin{axis}[
	x tick label style={/pgf/number format/1000 sep=},
	ylabel=MB/s,
	enlargelimits=0.01,
	legend style={at={(0.5,-0.1)},anchor=north,legend columns=-1},
	ybar interval=0.7,
  symbolic x coords={R, wc, grep, wc -l, cat, Run 372, Run 1, end}
]
\addplot coordinates {(R,6.0) (wc,182.6) (grep,261) (wc -l,852) (cat,878) (Run 372,908) (Run 1,1029) (end,0) };

% \legend{MB/s}

\end{axis}
\end{tikzpicture}

\caption{Throughput comparisons of Icicle against existing R code and standard Unix utilities}
\label{fig:bench:other}
\end{figure}


\begin{figure}

\begin{tikzpicture}
\begin{axis}[
	x tick label style={/pgf/number format/1000 sep=},
	ylabel=MB/s,
  ymin=800, ymax=1100,
%  xmax=372,
  xlabel=Number of queries,
	enlargelimits=0.01,
	legend style={legend columns=-1},
%	legend style={at={(0.5,-0.1)},anchor=north,legend columns=-1},
%	ybar interval=0.7,
]

% bench/raw.txt
% Running with fewer queries
 \addplot coordinates {(0,1025.5) (1,1029.6) (48,978) (96,941) (144,943) (192,936) (240,904) (276,895.5) (324,881) (372,846.2) };

% bench/raw.txt
% Running with fewer queries, better distribution
\addplot coordinates { (0,1029) (31,1013) (62,1006) (93,998) (124,991) (155,941) (186,956) (217,933) (248,931) (279,915) (279,915) (310,919) (341,916) (372,908) };


% Fusion, no output
\addplot coordinates { (0,1001) (31,1038) (62,1025) (93,1031) (124,1006) (155,1015) (186,1020) (217,997) (248,1026) (279,1024) (310,1009) (341,1038) (372,1025) };

\legend{CSE, No CSE, dev/null};

\end{axis}
\end{tikzpicture}


\caption{Decrease in throughput as queries are added, with and without common subexpression elimination (CSE) over queries}
\label{fig:bench:queries}
\end{figure}


The table in figure~\ref{fig:bench:other} shows the throughput in megabytes per second.
We compared the throughput of several programs over the same 317GB dataset:
\begin{itemize}
\item our original R implementation (R);
\item Icicle with equivalent queries (Run 372);
\item Icicle with a single query computing the mean (Run 1);
\item finding empty lines with @grep "^$"@;
\item counting characters, words and lines with @wc@;
\item counting only lines with @wc -l@; and
\item reading and throwing away the results with @cat > /dev/null@.
\end{itemize}

We ran all of the unix utilities with unicode disabled using @LANG=C LC_COLLATE@ for maximum performance.
The computer we used for most of these was an Amazon EC2 @c3.8xlarge@ with 32 CPUs, 60GB of RAM, and striped, RAIDed SSD storage.
The R code requires an @i2.8xlarge@ with 244GB of memory, but we were unable to perform our other benchmarks on such a large machine.
Icicle significantly outperformed R, @grep@ and @wc@, and was roughly on par with @wc -l@ and @cat@.
Although the computer had 32 CPUs, Icicle was the only benchmark we ran that used multiple CPUs: the other benchmarks were single threaded.

Figure~\ref{fig:bench:queries} shows how Icicle scales in throughput as more queries are added.

