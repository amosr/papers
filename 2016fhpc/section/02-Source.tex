%!TEX root = ../Main.tex
\section{Icicle}
\label{s:IcicleSource}

\input{figures/Source-Grammar.tex}
\input{figures/Source-Type.tex}
\input{figures/Source-Eval.tex}

The grammar for Icicle is given in figure~\ref{fig:source:grammar}.
Value types are given in $T$ and can be numbers, booleans or maps.
Modal types, $\TauMode$ can be a pure value type, or a modality associated with a value type.
Function types, $\TauFun$, can be either a non-function modal type, or a function from modal type to function type.
As Icicle is a first-order language, function types are not value types.

The table definition in $\mi{Table}$ gives a table name and the names and types of columns.
$\mi{Exp}$ defines the expressions used as query and function bodies.
$\mi{Prim}$ defines the primitive types.
Function definitions are given in $\mi{Fun}$.
Query definitions are given in $\mi{Query}$.
$\mi{Top}$ is the top-level program, which specifies a table, the set of function bindings, and the set of queries.

Expressions can be variable names, primitive operators such as addition and division, function application, let bindings, or query operations.
The query operations can define a fold such as a sum or a count, filtering according to a predicate, or grouping by a key.
Function application $\mi{x~Exp*}$ is the name of a top-level function applied to any number of arguments.
Primitive application $\mi{Prim~Exp*}$ is written prefix, but in the following we use infix-operator short-hand for convenience; for example, @(>) 0 1@ can be written as @0 > 1@.

Let-bindings allow a name to be used in place of the expression, in the rest of some query.
\begin{code}
let diff = open - close
in  mean diff
\end{code}

The fold form takes the name of the fold binding, and two expressions: the initial value and the update value for each element.
Count can be expressed by the following fold:
\begin{code}
fold count = 0 then count + 1
\end{code}

Filters take a predicate, which can be thought of as a stream of booleans, and a query to perform for the satisfying values.
The following query will count the number of entries where the open price exceeds the close price.
\begin{code}
filter open > close in count
\end{code}

Filters correspond to @WHERE@ clauses in SQL, except that the predicate does not apply to the entire top-level query, only to the subquery.
This makes it slightly easier to define queries like the proportion of entities satisfying some predicate to all entities: 
\begin{code}
(filter open > close in count) / count
\end{code}

Group takes a key to group by, and a query to perform on the values of each grouping.
This query groups the entries into buckets of the difference between open and close, and counts the number of records in each bucket.
\begin{code}
group ((open - close) / 100) in count
\end{code}

$\mi{Fun}$ is used for function definitions.
The @count@ function takes no arguments and returns a fold counting the number of elements.
\begin{code}
function count
 = fold c = 0 then c + 1;
\end{code}

Sum can be defined taking a single argument, @value@, which is the elements to compute the sum of.
It returns a fold that starts at zero and for every element, increases the old sum by @value@.
\begin{code}
function sum (value : Element Int)
 = fold s = 0 then s + value;
\end{code}

The definition of mean is just the sum divided by the count.
\begin{code}
function mean (value : Element Int)
 = sum value / count;
\end{code}

$\mi{Query}$ defines an aggregate query which must return a single value, and gives it a name:
\begin{code}
query count_all_records = count;
\end{code}

Finally, $\mi{Top}$ puts all these together with a table definition, function definitions, and query definitions in one place.
All queries here operate over the same input table, and will be fused into a single loop over the input data.



\subsection{Single-pass restriction}

All Icicle queries must execute in a single pass over the data, as reading the data multiple times is expensive. 
Ensuring that queries only require a single pass over the data also allows us to resume queries from where they left off, when new data arrives.

In order to ensure that queries can be executed as a single pass, we use a modal type system inspired by staged computation\cite{davies2001modal}.
We introduce two modalities, called @Element@ and @Aggregate@, which denote when computations must occur.

The @Element@ modality means that a computation is defined for each record in the table, or element in the stream.
Each column in the table is @Element@; for example the @open@ column has type @Element Int@, meaning that for each record in the column there is an @Int@.
This can be thought of as being represented by a stream of values.

The @Aggregate@ modality means that a computation is available only after all records in the table have been seen, or after the end of the stream.
These are used for the results of folds.
When computing count, the final value is not known until all the records have been seen, so @count@ has type @Aggregate Int@.

The modalities are automatically boxed and unboxed, so that if a function expecting a pure value is passed an @Element@ computation, the result of the function becomes an @Element@ computation too.
For example, if @open : Element Int@, then @open == 1@ has type @Element Bool@.
@Element@ and @Aggregate@ are mutually exclusive: a computation cannot be both an element and an aggregate.

Here is an example to highlight one benefit of tracking these modalities.
Suppose we have a table with two fields: key and value.
We wish to find the mean of values whose key matches the most recent key.
Someone new to stream programming might write the following query, forgetting that they can only read the data once: first find the most recent key, then go back and filter all the data to only those with the right key.

\begin{code}
   let k    = newest key
in let avg  = filter (key == k) in mean value
in avg
\end{code}

This will not typecheck in Icicle, as @newest key@ has the modality @Aggregate@, while the filter condition @key == k@ attempts to check if an @Element@ is equal to an @Aggregate@.

This query could be translated to another synchronous streaming language such as {\sc Lustre}\cite{halbwachs1991synchronous}, and would typecheck.
However, the semantics of the translated query will not be what was originally desired.
Rather than checking each key against the most recent key as-of the end of the stream,
it will check each key against the current most recent key, which is the key itself.
This means the filter predicate is always be true, and thus returns the mean of the entire stream.
These semantics may be surprising to the novice stream programmer.

Hopefully our programmer is not too discouraged by the @Element /= Aggregate@ type error, and pushes on.
They now know that their query, as formulated, requires multiple passes over the data: the problem now is to reformulate it as a single pass.
With a little ingenuity, we can rewrite it as such: we can group by the key and perform the mean for each group.
After the group, we can perform a lookup by the most recent key.
This requires storing and computing the means for all keys despite only needing one at the end, so we are assuming that the number of keys are bounded in some way.

\begin{code}
   let k    = newest key
in let avgs = group  key in mean value
in mapLookup avgs k
\end{code}

\subsection{Bounded buffer restriction}
\label{s:IcicleSource:bounded}
As Icicle is designed to be a streaming language, the amount of data to be streamed may not necessarily fit in memory.
Any operation which requires a buffer must be bounded in size, as an unbounded buffer could potentially grow too large to fit in memory.

Here is an example stream program that requires unbounded buffering:
it takes an input stream @xs@, and filters it into those above zero, and those below zero.
These two filtered streams are then joined together pairwise, so the first positive element is paired with the first negative element, and so on.
The program, written in Haskell:

\begin{code}
zip (filter (>0) xs) (filter (<0) xs)
\end{code}

This program requires an unbounded buffer: if the input stream contains ten positive values followed by one negative value, all ten of the positive values must be held onto until the negative value is seen.
Similarly, if the entire stream is positive, all of the elements must be retained until the very end, just in case a negative value shows up.

In Icicle, we outlaw this kind of program by implicitly threading the input stream through operations.
The streams themselves are not materialised in the program: stream operations like @fold@ and @filter@ do not take parameters of the streams, but instead operate on the context stream.
By removing the explicit stream parameter, stream elements can only be joined from the same context, when both elements are available.

For example, in the query @filter p in mean value@, the @mean value@ is only applied to stream values which satisfy the predicate @p@.

This is a different approach than existing synchronous streaming languages\cite{mandel2010lucy} and flow fusion\cite{lippmeier2013data}, which perform clock analysis or `rate inference'.
Here, streams are given clock types denoting when the streams have elements available, and only streams with the same clock can be zipped together.
Filters produce a stream with a different output clock to its input, so different filters cannot be zipped together.
Our system requires no clock analysis, but is less expressive as streams are not `first class'.

\subsection{Type system}

The typing rules for Icicle are given in figure~\ref{fig:source:type:exp}.
Judgment $\Typecheck{\Gamma}{\mi{Exp}}{\TauMode}$ means that under context $\Gamma$, the expression has a given value type and modality.
Judgment $\TypecheckP{\mi{Prim}}{\TauFun}$ associates a primitive with its function type.
Judgment $\TypecheckApp{\TauFun}{\ov{\TauMode}}{\TauMode}$ is used for automatically boxing and unboxing modalities in function application: a function type applied to a list of argument value types and modalities results in a single value type and modality.
Judgment $\TypecheckS{\Gamma}{\mi{Def}}{\Gamma}$ takes an input environment and function or query, and returns an environment containing the function or query name and its type.
Finally, $\TypecheckS{}{\mi{Top}}{\Gamma}$ takes a top-level definition with a table, functions and queries, and returns a context containing only the types of the queries.

Rule TcVar performs variable lookup in the context, while TcNat shows that number constants have type $\NN$.

Rule TcBox shows that a pure value can be converted to an @Element@ or @Aggregate@, by either replicating it along the stream or returning it at the end of the stream.
The premise $\tau~\in~T$ requires that $\tau$ is a pure value, rather than an @Element@ or an @Aggregate@.

Rules TcPrimApp and TcFunApp find the result type of primitive and function application.
They first find the type of the primitive or function, and the types of the arguments, then use the application boxing/unboxing judgment $\TypecheckApp{\TauFun}{\ov{\TauMode}}{\TauMode}$ to find the return type and modality of the application.
The set-comprehension syntax $\{e_i\}$ means that this rule allows any number of arguments.

Rule TcLet is a standard let rule, finding the type of the bound expression and inserting it into the context when typechecking the remaining expression.

Rule TcFold finds the type of a @fold@ expression.
The initial value of the fold must be pure.
The update has the current value of the fold inserted in its context as an @Element@, and the update must be an @Element@.
The final return type of the fold is an @Aggregate@.

Rule TcFilter requires its predicate to be a stream of booleans ($@Element@~\BB$).
The rest of the filter must be an @Aggregate@; returning an @Element@ stream with a different rate is not allowed, as explained in\sref{s:IcicleSource:bounded}.
Rule TcGroup is similar to filter, except returning a @Map@.

The rules PrimArith, PrimRel and PrimLookup give function types for each primitive.

The next judgment shows how function application boxes and unboxes modalities.
This judgment takes a function type and all its arguments, and returns the result type of the function.

Rule AppArgs applies when a function is applied to exactly the right arguments, and simply returns the result of the function.

Rule AppRebox is used when a pure function is applied to a non-pure value.
If the arguments and the result type are all pure, then the function can be applied to arguments of a particular modality, and the return type becomes that modality too.
Note that rule TcBox can also apply implicit boxing here; when only some of the arguments are non-pure, the pure ones can be boxed.
These rules are tricky to get right; an earlier version allowed applying a modal argument to a pure argument if the return type fitted that mode, which can lead to giving the results of folds as initial values for folds.



Rule CheckFun inserts the argument types into the context and returns a new context with the function's type, which is the type of the arguments to the result type.

Rule CheckQuery checks a query's types, and makes sure it returns an @Aggregate@.

Finally, rule CheckTop checks a whole top-level program.
It first creates a new context with the table columns bound to @Element@s, and typechecks each function.
Then, each query is checked, with the table columns and all functions available.
The final returned context is all query names and their types.

\subsection{Evaluation}

The evaluation rules for Icicle are given in figure~\ref{fig:source:eval}.

We define simple values $V$ as numbers, booleans, maps of values, or tuples of values.
Next, stream values $V'$ correspond to modalities: pure computations are represented as a single value;
@Element@s as a stream transformer from a heap to a value;
and @Aggregate@s as folds defining an initial state and update and extract functions.

The evaluation rules operate over a slightly modified expression type, $E'$, which is an $\mi{Exp}$ with $V'$ as values instead of just simple values and natural numbers.
This is to simplify typechecking over $V'$, and as $V'$ only appear in restricted form for evaluation.
We also assume that all functions have been inlined before evaluation.

The judgment $\SourceStepX{E'}{V'}$ defines a big-step evaluation relation.

(EVal) applies when the expression is alread a stream value.
This corresponds, roughly, to the (TcNat) typechecking rule.

(EBoxStream) and (EBoxFold) convert a pure value to a stream and fold, respectively.
The stream transformer ignores the input stream and returns the pure value.
The fold has a unit accumulator, and the extract function simply returns the pure value.

Next (EPrimValue), (EPrimStream) and (EPrimFold) evaluate primitive applications.
For pure values, the values are unboxed and the primitive is applied.
For streams, a new stream is created and the input heap applied to all streams before applying the primitive.
Folds are the most complicated: the fold accumulators are tupled together, for example $(\times_i~z_i)$ being the tuple of all the initial accumulators.
At each step, the accumulators for all the input folds are updated with their own update function, and the input heap.
Finally, the extract function extracts the final values of the input folds, and applies the primitive.

The (ELet) rule is fairly standard, evaluating the bound to a value, substituting it in, and evaluating the rest.

(EFold) is one of the most complex rules, as it introduces recursion to the streams.
It evaluates its initial to a pure value, $z'$, which is used as the initial state for the fold.
In the update expression, the fold binding $x$ is substituted with a stream that looks up $x$ in its input heap.
This action-at-a-distance is necessary because the value of $x$ depends on the value of the fold, which is not known yet.
Finally, the return fold is constructed, which binds $x$ to the current fold accumulator.
The extract function for this is the identity, simply returning the accumulator.

(EFilter) evaluates its predicate to a stream of booleans, and the rest to a fold.
The result is a fold which only applies the update function when the predicate stream is true.

(EGroup) is rather similar to filter, except that it returns a map.
Here, we start with an empty map of bottoms.
At every update step, we find the key and look up the current accumulator for that key in the map, or the fold initialiser if it is bottom.
We then apply the update function to the accumulator, and update that key in the map.
For the extract function, we simply apply the extract to each element of the map.

In order to execute these evaluation rules on an actual input table, a few steps are necessary.
First, variables referring to column names such as @open@ or @close@ must have stream values substituted in, so that the actual value will be looked up in the input heap.
For example, $e[@open@~:=~@Stream@~(\lam{\sigma} \sigma~@open@)]$.
Next, the query must be converted to a @Fold@.
Then, for each record, the update function would be called with an input heap binding the record values: $\{@open@ = 1, @close@ = 2\}$ and so on.
This occurs for every record in the table.
Finally, the extract function is called, which gives the result of the query.

