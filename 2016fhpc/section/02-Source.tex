%!TEX root = ../Main.tex

\eject
% ---------------------------------------------------------
\section{Elements and Aggregates}
To allow incremental computation all Icicle queries must execute in a single pass over the input stream.
Of course, not all queries \emph{can} be executed in a single pass: the key examples are queries that require random access indexing, or otherwise need to access data in an order different to what the stream provides.
However, as we saw in the introduction, although a particular \emph{algorithm} may be impossible to evaluate in a streaming fashion, the desired \emph{value} may well be computable, if only we had a different algorithm.
Here is the unstreamable example from the introduction again:
\begin{code}
  table kvs { key : Date; value : Real }
  query avg = let k = last key
              in  filter (key == k) of mean value;
\end{code}

The problem with this is that the value of @last key@ is only availble once we have reached the end of the stream, but the @filter@ operation needs this value to process the very first element in the same stream.
We distinguish between these two access patterns by giving them different names: we say that @last key@ is an \emph{aggregate}, because to compute it we must have consumed the \emph{entire stream}, whereas the filter predicate is an \emph{element}-wise computation because it only needs access to the current element in the stream.

The trick to compute our average in an incremental fashion is to recognise that @filter@ is selecting a particular subset of values from the input, but the value computed from this subset depends only on the values in that subset, and no other information. Instead of computing the mean of a single subset whose identity is only known at the end of the stream, we can instead compute the mean of \emph{all possible subsets}, and return the required one once we know what that is:
\begin{code}
  table kvs { key : Date; value : Real } 
  query avg = let k    = last  key in
              let avgs = group key of mean value
              in  lookup k avgs
\end{code}

Here we use the @group@ construct to assign key-value pairs to groups as we obtain them, and compute running mean of the values of each group. The @avgs@ value becomes a map of group keys to their running means. Once we reach the end of the stream we will have access to the last key and can lookup the final result.


% ---------------------------------------------------------
\subsection{The Stage Restriction}
To ensure that Icicle queries can be evaluated in a single pass, we use a modal type system inspired by staged computation~\cite{davies2001modal}. We use two modalities, @Element@ and @Aggregate@. Values of type @Element@~$\tau$ are taken from input stream on a per-element basis, whereas values of type @Aggregate@~$\tau$ are available only once the entire stream has been consumed. In the expression @(filter (key == k) of mean value)@, the variable @key@ has type @Element Date@ while @k@ has type @Aggregate Date@. Attempting to compile the unstreamable query in Icicle will produce a type error complaining that elements cannot be compared with aggregates.

Note that the types of pure values such as constants are automatically promoted to the required modality, for example, if we have @open == 1@ and @open : Element Int@ then the constant @1@ is automatically promoted to have type @Element Int@ as well.


% ---------------------------------------------------------
\subsection{Finite Streams and Synchronous Data Flow}
In contrast to synchronous data flow languages such as {\sc Lustre}~\cite{halbwachs1991synchronous}, all streams processed by Icicle are conceptually finite in length. Icicle is fundamentally a query language, which queries finite tables of data held in a non-volatile store, but does so in a streaming manner. Lustre operates on conceptually infinite streams, such as those found in real-time control systems (like to fly airplanes). In Icicle, the ``last'' element in a stream is the last one that appears in the table on disk. In Lustre, the ``last'' element in a stream is the one that was most recently received. If the unstreamable query from \S2 was converted to Lustre syntax then it would execute, but the filter predicate would compare the last key with the most recent key from the stream, which is the key itself. The filter predicate would always be true, and the query would return the mean of the entire stream. Applying the Icicle type system to our queries imposes the natural stage restriction associated with finite streams, so there are distinct ``during'' (element) and ``after'' (aggregate) stages.


% ---------------------------------------------------------
\subsection{Incremental Update}
Suppose we query a large table and record the result. Tomorrow morning we receive more data and add it to the table. We would like to update the result without needing to process all data from the start of the table. We can do this by remembering the values of all intermediate aggregates that were computed in the query, and updating them as new data arrives. In the @avg@ example from \S2 these aggregates are @k@ and @avgs@. 

We can also provide impure contextual information to the query such as the current date, by assigning it an aggregate type. As element-wise computations cannot depend on aggregate computations, we ensure that reused parts of an incremental computation are the same regardless of which day they are executed.


% ---------------------------------------------------------
\subsection{Bounded buffer restriction}
\label{s:IcicleSource:bounded}
Icicle queries process tables of arbitrary size that may not fit in memory. Due to this, each query must execute without requiring buffer space proportional to the size of the input. As a counter example, here is a simple function which cannot be applied without reserving a buffer of the same size as the input:
\begin{code}
    unbounded (xs : Stream Int)
     = zip (filter (> 0) xs) (filter (< 0) xs)
\end{code}

This function takes an input stream @xs@, and pairs the elements that are greater than zero with those that are less than zero.This computation requires an unbounded buffer because if the stream contains $n$ positive values followed by $n$ negative values, then all the positive values must be retained before any output can be produced. 

Icicle is designed so that queries that would require unbounded buffering cannot be written, with one major caveat that we will discuss in a moment. In Icicle, the stream being processed (such as @xs@ above) is implicit in the defined query, and is never named explicitly. Constructs such as @filter@ and @fold@ do not take the name of the stream as an argument, but instead operate on the stream defined in the context. In the example Icicle queries presented so far, the name of the table (stream) is not mentioned in the body of the query, and serves as a comment only. Icicle language constructs describe \emph{how elements from the stream should be aggregated}, but the order in which those elements are aggregated is implicit, rather than being definable by the body of the query. In the expression @filter p of mean value@, the term @mean value@ is applied to stream values which satisfy the predicate @p@, but the values to consider are supplied by the context.

Finally, note that our major caveat is that the @group@ construct we used in \S2 uses space proportional to the number of distinct \emph{keys} in the input stream.
For our applications the keys are commonly company names, customer names, and days of the year. Our production system knows that these types are bounded in size and that maps from keys to values will fit easily in memory. Attempting to group by values of a type with an unbounded number of members, such as a @Real@ or @String@ results in a compile-time warning.

% Grouping by types with an unbounded number of members, such as @Real@ or @String@ can be undesirable, and we wish to outlaw this in a future version of our production compiler.

% BEN: "Can be undesirable" is too imprecise. What will happen is that an entry will be added to the map for every key, and if there are more keys than will fit in memory then the query will run out of memory. Surely adding the mentioned warning to the compiler would not be difficult? If it's easy then saying it's done sounds much better than "we intend to". We need to have a solid story about this point. One of the key contributions of Icicle is that it does not require ``unbounded buffering'', but if you're grouping by arbitrary values from the input stream then it clearly does.


\eject
% ---------------------------------------------------------
\section{Source Language}
\label{s:IcicleSource}

\input{figures/Source-Grammar.tex}
\input{figures/Source-Type.tex}
\input{figures/Source-Eval.tex}

The grammar for Icicle is given in Figure~\ref{fig:source:grammar}. Value types $T$ include numbers, booleans or maps. Modal types $\TauMode$ include the pure value types, and modalities associated with a value type. Function types $\TauFun$ include non-function modal types, or a function from modal type to function type. As Icicle is a first-order language, function types are not value types.

Table definitions $\mi{Table}$ define a table name and the names and types of columns. Expressions $\mi{Exp}$ can be variable names, constants, applications of primitives or functions. The @fold@ construct defines the name of an accumulator, the expression for the initial value, and the expression used to update the accumulator for each element of the stream. The @filter@ construct defines a predicate and an expression to accumulate values for which the predicate is true. The @group@ construct defines an expression used to determine the key for each element of the stream, and an expression to accumulate the values that share a common key.

$\mi{Prim}$ defines the primitive operators.
Function and query definitions are given in $\mi{Def}$. $\mi{Top}$ is the top-level program, which specifies a table, the set of function bindings, and the set of queries. All queries in a top-level program process the same table.


% ---------------------------------------------------------
\subsection{Type system}
The typing rules for Icicle are given in Figure~\ref{fig:source:type:exp}.
The judgment form $\Typecheck{\Gamma}{\mi{Exp}}{\TauMode}$ associates an expression $\mi{Exp}$ with its type $M$ under context $\Gamma$.
The judgment form $\TypecheckP{\mi{Prim}}{\TauFun}$ associates a primitive with its function type.
The judgment form $\TypecheckApp{\TauFun}{\ov{\TauMode}}{\TauMode}$ is used for automatically boxing and unboxing modalities in function application: a function type applied to a list of argument value types and modalities results in a single value type and modality.
The judgment form $\TypecheckS{\Gamma}{\mi{Def}}{\Gamma}$ takes an input environment and function or query, and returns an environment containing the function or query name and its type.
Finally, $\TypecheckS{}{\mi{Top}}{\Gamma}$ takes a top-level definition with a table, functions and queries, and returns a context containing only the types of the queries.

Rule TcVar performs variable lookup in the context, while TcNat shows that number constants have type $\NN$. Rule TcBox assigns a pure value either @Element@ or @Aggregate@ type. 

% shows that a pure value can be converted to an @Element@ or @Aggregate@, by either replicating it along the stream or returning it at the end of the stream.
% The premise $\tau~\in~T$ requires that $\tau$ is a pure value, rather than an @Element@ or an @Aggregate@.

Rules TcPrimApp and TcFunApp produce the type of the a primitive or function applied to its arguments.

% find the result type of primitive and function application.
% They first find the type of the primitive or function, and the types of the arguments, then use the application boxing/unboxing judgment $\TypecheckApp{\TauFun}{\ov{\TauMode}}{\TauMode}$ to find the return type and modality of the application.
% The set-comprehension syntax $\{e_i\}$ means that this rule allows any number of arguments.

Rule TcLet is a standard let rule, finding the type of the bound expression and inserting it into the context when typechecking the remaining expression.

Rule TcFold finds the type of a @fold@ expression.
The initial value of the fold must be pure.
The update has the current value of the fold inserted in its context as an @Element@, and the update must be an @Element@.
The final return type of the fold is an @Aggregate@.

Rule TcFilter requires its predicate to be a stream of booleans ($@Element@~\BB$).
The rest of the filter must be an @Aggregate@; returning an @Element@ stream with a different rate is not allowed, as explained in\sref{s:IcicleSource:bounded}.
Rule TcGroup is similar to filter, except returning a @Map@.

The rules PrimArith, PrimRel and PrimLookup give function types for each primitive.

The next judgment shows how function application boxes and unboxes modalities.
This judgment takes a function type and all its arguments, and returns the result type of the function.

Rule AppArgs applies when a function is applied to exactly the right arguments, and simply returns the result of the function.

Rule AppRebox is used when a pure function is applied to a non-pure value.
If the arguments and the result type are all pure, then the function can be applied to arguments of a particular modality, and the return type becomes that modality too.
Note that rule TcBox can also apply implicit boxing here; when only some of the arguments are non-pure, the pure ones can be boxed.
These rules are tricky to get right; an earlier version allowed applying a modal argument to a pure argument if the return type fitted that mode, which can lead to giving the results of folds as initial values for folds.



Rule CheckFun inserts the argument types into the context and returns a new context with the function's type, which is the type of the arguments to the result type.

Rule CheckQuery checks a query's types, and makes sure it returns an @Aggregate@.

Finally, rule CheckTop checks a whole top-level program.
It first creates a new context with the table columns bound to @Element@s, and typechecks each function.
Then, each query is checked, with the table columns and all functions available.
The final returned context is all query names and their types.


\subsection{Evaluation}

The evaluation rules for Icicle are given in figure~\ref{fig:source:eval}.

Definition $V$ are simple values of numbers, booleans, maps and tuples.
Definition $V'$ are stream values that correspond to modalities: pure computations are represented as a single value;
@Element@s as a stream transformer from a heap to a value;
and @Aggregate@s as folds defining an initial state and update and extract functions.
Definition $M'$ are the modalities themselves without value types.


The evaluation rules operate over a slightly modified expression type, $E'$, which is an $\mi{Exp}$ with $V'$ as values instead of just simple values and natural numbers.
This is to simplify typechecking over $V'$, and as $V'$ only appear in restricted form for evaluation.
We also assume that all functions have been inlined before evaluation.

The judgment $\SourceStepX{M'}{E'}{V'}$ defines a big-step evaluation relation.
The rules below use the modality in deciding whether pure values should be converted to streams or folds.

Rule EVal applies when the expression is alread a stream value.
This corresponds roughly to the TcNat typechecking rule.

Rules EBoxStream and EBoxFold convert a pure value to a stream and fold respectively, when the environment modality is not pure.
The stream transformer ignores the input stream and returns the pure value.
The fold has a unit accumulator, and the extract function simply returns the pure value.

Rules EPrimValue, EPrimStream and EPrimFold evaluate primitive applications.
For pure values, the values are unboxed and the primitive is applied.
For streams, a new stream is created and the input heap applied to all streams before applying the primitive.
Folds are the most complicated: the fold accumulators are tupled together, for example $(\times_i~z_i)$ being the tuple of all the initial accumulators.
At each step, the accumulators for all the input folds are updated with their own update function, and the input heap.
Finally, the extract function extracts the final values of the input folds, and applies the primitive.

Rule ELet rule is fairly standard, evaluating the bound to a value, substituting it in, and evaluating the rest.
The only difference is we inspect the type of the bound expression to find the modality, then evaluate the bound under that modality.
The rest of the expression is evaluated under the original modality.

Rule EFold is one of the most complex, as it introduces recursion to the streams.
It evaluates its initial to a pure value, $z'$, which is used as the initial state for the fold.
In the update expression, the fold binding $x$ is substituted with a stream that looks up $x$ in its input heap.
This sleight of hand is necessary because the value of $x$ depends on the value of the fold, which is not known yet.
Finally, the return fold is constructed, which binds $x$ to the current fold accumulator.
The extract function for this is the identity, simply returning the accumulator.

Rule EFilter evaluates its predicate to a stream of booleans, and the rest to a fold.
The result is a fold which only applies the update function when the predicate stream is true.

Rule EGroup is rather similar to filter, except that it returns a map.
Here, we start with an empty map of bottoms.
At every update step, we find the key and look up the current accumulator for that key in the map, or the fold initialiser if it is bottom.
We then apply the update function to the accumulator, and update that key in the map.
For the extract function, we simply apply the extract to each element of the map.

In order to execute these evaluation rules on an actual input table, a few steps are necessary.
First, variables referring to column names such as @open@ or @close@ must have stream values substituted in, so that the actual value will be looked up in the input heap.
For example, $e[@open@~:=~@Stream@~(\lam{\store} \store~@open@)]$.
Next, the query must be converted to a @Fold@.
Then, for each record, the update function would be called with an input heap binding the record values: $\{@open@ = 1, @close@ = 2\}$ and so on.
This occurs for every record in the table.
Finally, the extract function is called, which gives the result of the query.

