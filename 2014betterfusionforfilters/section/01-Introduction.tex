%!TEX root = ../Main.tex
\section{Introduction}

Recent work on data flow fusion~\cite{lippmeier2013flow} shows how to compile a specific class of data flow programs into \emph{single}, efficient imperative loops.
This process of ``compilation'' is equivalent to performing array fusion on a combinator based functional array program, as per related work on stream fusion~\cite{coutts2007streamfusion} and delayed arrays~\cite{keller2010repa}.
The key benefits of data flow fusion over prior work are that 1) it can fuse programs that use branching data flows where a produced array is consumed by several consumers, and 2) complete fusion into a single loop is guaranteed for all programs that operate on the same size input data, and contain no fusion-preventing dependencies between combinators.


Ultimately, though, we cannot expect \emph{all} programs to be fused into single imperative loops, as most contain fundamental data dependencies that prevent complete fusion. For example:
\begin{code}
  normalize :: Array Int -> Array Int
  normalize xs = let sum = fold (+) 0 xs
                 in  map (/ sum) xs
\end{code}

If we wish to divide every element of an array by the sum of all elements, then it seems we are forever destined to compute the result using two loops: one to determine the sum, and one to divide the elements.
The application of @fold@ demands all elements of its source array, and we cannot produce any elements of the result array until we know the value of @sum@.

However, many programs \emph{do} contain opportunities for fusion, if we only knew which opportunities to take.
The following function is similar to the one from before, except that it offers several unique \emph{but mutually exclusive} approaches to fusion.
A diagram of the data flow graph for this example is on the next page.

\begin{code}
 normalize2 :: Array Int -> Array Int
 normalize2 us
  = let sum1 = fold   (+) 0 us      (A1) (S1) (I1)
        gts  = filter (>0)  us      (A1) (S2) (I1)
        sum2 = fold   (+) 0 gts     (A1) (S2) (I2)
        nor1 = map  (/sum1) us      (A2) (S3) (I3)
        nor2 = map  (/sum2) us      (A2) (S4) (I3)
    in (nor1, nor2)
\end{code}
Our approach, @A1-A2@, requires only two loops for this particular case.
Stream fusion (@S1-S4@) is unable to fuse @sum1@ and @gts@ together, as there is nothing to be inlined between them, so requires four whole traversals of the input array, @us@.
Similarly, existing imperative solutions (@I1-I3@) are unable to fuse @gts@ and @sum2@ together, as the iteration size is completely different.

Our contributions are as follows:

% NOTE: This set of bullets needs to fit on the first page, without spilling to the second.
\begin{itemize}
\item   
We use existing integer linear programming clustering formulations such as Megiddo\cite{megiddo1998optimal} and Darte\cite{darte2002contraction},
and modify them to express constraints on fusing loops of different sizes if they can be fused into generator loops of the same size.
\TODO{ref}
% We present an algorithm to cluster the operators of a data flow graph into sub-graphs that can be fused into individual imperative loops.
% We express the constraints on fusion as an integer linear program, and use an off-the-shelf solver to compute a valid clustering. \TODO{ref}

\item
We present a simplification to constraint generation that is also applicable to some existing integer linear programming formulations such as Megiddo's,
where constraints between two nodes need not be generated if there exists a fusion-preventing path between the two. \TODO{ref}

\item
Our constraint system also encodes a total ordering on the cost of clusterings, expressed using weights on the integer linear program.
For example, we encode that memory traffic is more expensive than loop overheads, so given a choice between the two, the memory traffic will be reduced.
\TODO{ref}

\item
We present benchmarks of our algorithm applied to several common programming patterns, and to several pathological examples.
Our algorithm is complete and yields good results in practice, though if array sizes are unknown, an optimal solution is uncomputable in general. \TODO{ref}
\end{itemize}

The reduction of the clustering problem to integer linear programming was previously described by\cite{megiddo1998optimal}, though they do not consider filtering operations.


% We must also decide which clustering is the `best' or most optimal. One obvious criterion for this is the minimum number of loops, but there may even be multiple clusterings with the minimum number of loops. In this case, the number of required manifest arrays must also be taken into account. 

% Consider the @normaliseInc@ function below; it requires at least two loops, since @sum@ must be computed from the entire array before any of the output array @norm@ can be filled. Scheduling @incs@ and @sum@ into a single loop, then @norm@ into another loop only requires two loops, but requires @incs@ to be stored in a manifest array before being read again by @norm@. If we were to schedule @sum@ in its own loop and @incs@ and @norm@ together, we would only require one array to be filled - the output array.

% Combinators are a natural and composable way of expressing array programs, with many correctness benefits over imperative programs. If compiled naively, however, executing the combinator program will often require many more loops and intermediate arrays than an imperative program written by hand. These intermediate arrays can often be removed by merging several combinators into a single loop. This merging is known as \emph{fusion}.

% Existing fusion systems for Haskell such as stream fusion~\cite{coutts2007streamfusion}, tend to cleverly reuse compiler optimisations such as inlining and rewrite rules, to fuse combinators without having to modify the compiler itself. This approach has the advantage of simplicity, but is inherently limited in the amount of fusion it can perform. However, for scientific applications where efficiency is important, we cannot afford these limitations. We have instead implemented our fusion system as a compiler plugin, which gives us total control of when fusion will occur.

% As real programs contain tens or hundreds of individual operators, performing an exhaustive search for an optimal clustering is not feasible, and greedy algorithms tend to produce poor solutions. 

