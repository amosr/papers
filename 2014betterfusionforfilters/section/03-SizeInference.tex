%!TEX root = ../Main.tex
\section{Size inference}
Before performing fusion proper, we must infer the relative sizes of each array in the program. We achieve this with a simple constraint based inference algorithm, which we discuss in this section. Size inference has been previously described in the context of array fusion by Chatterjee~\cite{chatterjee1991size}. In constrast to our algorithm, \cite{chatterjee1991size} does not support size changing functions such as filter.

Although our constraint based formulation of size inference is reminiscent of type inference for HM(X)~\cite{odersky1999type}, there are important differences. Firstly, our type schemes include existential quantifiers, which represent the fact that the sizes of arrays produced by filter operations are unknown in general. This is also the case for @generate@, where the result size is data dependent. HM(X) style type inferences uses the $\exists$ quantifier to bind local type variables in constraints, and existential quantifiers do not appear in type schemes. Secondly, our types are first order only, as program graphs cannot take other program graphs as arguments. Provided we generate the constraints in the correct form, solving them is straightforward.


% Our formulation of size inference is very similar to type inference, particularly the exposition of HM(X). First, constraints are generated for the bindings, such as equality among two sizes, the conjunction of two constraints, and existentials. If these constraints can be solved, the equality constraints are used to group the sizes into equivalence classes. Otherwise if the constraints are unable to be solved, the program cannot be assured to require no runtime size checks and will not be fused by our system. After the constraints are generated and solved, each combinator is given an iteration size. Two loops with the same iteration size can be fused together.

% While it \emph{is} technically possible to fuse loops of different sizes, it requires extra complexity to find the maximum of the loop sizes, and additional branches are required to only execute the smaller loop fewer times. For simplicity, we do away with this added complexity and only support fusion of equal sized loops. \emph{Size inference} is performed on the combinators to infer as much information as possible about the sizes of the resulting loops.

% A more in-depth analysis has also been explored by Jay~\cite{jay1996shape} in the form of shape inference, which is built from primitives such as @cons@ and @nil@. Since we have fewer primitives and simpler goals, our size inference does not need to be as complicated as Jay's shape inference.

% The $@map@_n$ combinators require all input arrays to be the same size, and their output is the same. Since @fold@s do not produce arrays, they have no constraints. @gather@ takes two arrays; the data and the indices. Its output size is the size of the indices. The size of a @filter@'s output is most interesting; the exact size is not known until after it has been executed, only that it is less than or equal to the input size. Similarly, the size of @external@ outputs is not known at all, and thus cannot be constrained.


% -----------------------------------------------------------------------------
\begin{figure}
\begin{tabbing}
MMMMMMMM \= MM  \= MM \= MMMMMM \= \kill
\textbf{Size Type}
\> $\tau$   \> @::=@  \> $k$                  \> (size variable)       \\
\>          \> $~|$   \> $\tau \times \tau$   \> (cross product)
\end{tabbing}

\begin{tabbing}
MMMMMMMM \= MM  \= MM \= MMMMMM \= \kill
\textbf{Size Constraint}
\> $C$      \> @::=@  \> $true$               \> (trivially true)      \\
\>          \> $~|$   \> $\tau = \tau$        \> (equality constraint) \\
\>          \> $~|$   \> $C \wedge C$         \> (conjunction)
\end{tabbing}

\begin{tabbing}
MMMMMMMM \= MM  \= MM \= MMMMMM \= \kill
\textbf{Size Scheme}
\> $\sigma$ \> @::=@  
        \> $\forall \ov{k}.~ \exists \ov{k}.~ (\ov{ x : \tau }) \to (\ov{x : \tau})$
\end{tabbing}

\caption{Sizes, Constraints and Schemes}
\label{f:constraints}
\end{figure}


\newcommand{\constr}[1]{\llbracket #1 \rrbracket}


% -----------------------------------------------------------------------------
\subsection{Size types, constraints and schemes}
\label{s:SizeTypes}
Figure~\ref{f:constraints} shows the grammar for size types, constraints and schemes. A size scheme is like a type constraint from Hindley-Milner type systems, except that it only mentions the size of each input array, instead of the element types as well.

A size may either be a variable $k$, or a cross product of two sizes. We use the latter to represent the result size of the @cross@ operator discussed in the previous section. Constraints maybe either be trivially $true$, an equality $\tau = \tau$, or a conjunction of two constraints $C \wedge C$. We refer to the trivially true and equality constraints as \emph{atomic constraints}. Size schemes relate the sizes of each input and output array. For example, the size scheme for the @normalize@ example from Figure~\ref{f:normalize-clusterings} is as follows:
$$@normalize@ ~:_s \forall k. (xs : k) \to (ys_1 : k,~ ys_2 : k)
$$

We write $:_s$ to distinguish size schemata from type schemata.

The existential quantifier appears in size schemes when the array produced by a filter or similar operator appears in the result. For example:
\begin{alltt}
   filterLeft \(:\sb{s}\,\forall\,k\sb{1}.\,\exists\,k\sb{2}.\,(xs\,:\,k\sb{1})\;\to\;(ys\sb{1}\,:\,k\sb{1},\,ys\sb{2}\,:\,k\sb{2})\)
   filterLeft xs
     = let ys1 = map (+ 1)   xs
           ys2 = filter even xs
       in (ys1, ys2)
\end{alltt}

The size scheme of @filterLeft@ shows that it works for input arrays of all sizes. The first result array has the same size as the input, and the second has some unrelated size.

Finally, note that size schemes form but one aspect of the type information that would be expressible in a full dependently typed language. For example, in Coq or Agda we could write something like:
\begin{alltt}
filterLeft : \(\forall\,k\sb{1}:\,\)Nat\(.\,\exists\,k\sb{2}:\,\)Nat\(.\) 
  Array \(k\sb{1}\) Float \(\to\) (Array \(k\sb{1}\) Float, Array \(k\sb{2}\) Float)
\end{alltt}

However, the type inference systems for fully higher order dependently typed languages typically require quantified types to be provided by the user, and do not perform the type generalization process. In our situation, we need automatic type generalization, but for a first order language only.


% -----------------------------------------------------------------------------
\subsection{Constraint Generation}
The rules for constraint generation are shown in Figure~\ref{f:ConstraintGeneration}. The top level judgment ~~$\SizeF{function}{\sigma}$~~ assigns a size scheme to a function. It does this by extracting size constraints and then solving them. This rule, along with the constraint solving process is discussed in the next section. The judgment ~~$\SizeB{\Gamma_1}{zs}{b}{\Gamma_2}{C}$ reads: ``Under environment~$\Gamma_1$, array variable $zs$ binds the result of $b$, producing a result environment $\Gamma_2$ and size constraints $C$''. The remaining judgment that extracts constraints from a list of bindings is similar. The environment $\Gamma$ has the following grammar:
$$
\Gamma~ @::=@ ~~\cdot ~~|~~ \Gamma,~ \Gamma ~~|~~ zs : k ~~|~~ k ~~|~~ \exists k
$$

As usual, $\cdot$ represents the empty environment and  $\Gamma,~ \Gamma$
environment concatenation. The element $zs : k$ records the size $k$ of some
array variable $zs$. A plain $k$ indicates that $k$ can be unified with other
size types when solving constraints, whereas $\exists k$ indicates a  \emph{rigid} size variable that cannot be unified with other sizes. We use the $\exists k$ syntax because this variable will also be existentially quantified if it appears in the size scheme of the overall function.

Note that the constraints are generated in a specific form, to make the constraint solving process easy. For each array variable in the program we generate a new size variable, like size $k_{zs}$ for array variable $zs$. These new size variables always appear on the \emph{left} of atomic equality constraints. For each array binding we may also introduce unification or rigid variables, and these appear on the \emph{right} of atomic equality constraints.

For example, the final environment and constraints generated for the @normalize@ example from Section~\ref{s:Introduction} are as follows:
$$
\begin{array}{ll}
       & x : k_{xs},~ gts : k_{gts},~ \exists k_1,~ k_2,~ k_3 
\\
\vdash & true 
        ~\wedge~  k_{gts} = k_1
        ~\wedge~  true
\\     &~~~~~~~~ 
          \wedge~  k_{xs}  ~= k_2
        ~ \wedge~  k_{ys1}  = k_2 
        ~ \wedge~  k_{xs}   = k_3
        ~ \wedge~  k_{ys2}  = k_3
\end{array}
$$


% -------------------------------------------------------------------
\subsection{Constraint Solving and Generalization}
The top-level rule in Figure~\ref{f:ConstraintGeneration} assigns a size scheme to a function by first extracting size constraints, before solving them and generalizing the result. In the rule, the solving process is indicated by $\textrm{SOLVE}$, and takes and environment and constraint, producing a solved environment and constraint. As the constraint solving process is both standard and straightforward, we only describe it informally.

Recall from the previous section that in our generated constraints all the size variables named after program variables are on the left of atomic equality constraints, while all the unification and existential variables are on the right. To solve the constraints we keep finding pairs of atomic equality constraints where the same variable appears on the left, unify the right of both of these constraints, and apply the resulting substitution to both the environment and original constraints. When there are no more pairs of constraints with the same variable on the left then the constraints are in solved form and we are finished.

During constraint solving, all unification variables mentioned in the environment can have other sizes substituted for them. In contrast, the rigid variables marked by the $\exists$ symbol cannot. For example, if we consider the constraints for @normalize@ mentioned before:
$$
\begin{array}{ll}
       & x : k_{xs},~ gts : k_{gts},~ \exists k_1,~ k_2,~ k_3 
\\
\vdash & true 
        ~\wedge~  k_{gts} = k_1
        ~\wedge~  true
\\     &~~~~~~~~ 
          \wedge~  k_{xs}  ~= k_2
        ~ \wedge~  k_{ys1}  = k_2 
        ~ \wedge~  k_{xs}   = k_3
        ~ \wedge~  k_{ys2}  = k_3
\end{array}
$$

Note that $k_xs$ is mentioned twice on the right of an atomic equality constraint, so we can substitute $k_2$ for $k_3$. Eliminating the duplicates, as well as the trivially $true$ terms then yields:
$$
\begin{array}{ll}
       & x : k_{xs},~ gts : k_{gts},~ \exists k_1,~ k_2 
\\
\vdash & k_{gts} = k_1
        ~\wedge~  k_{xs}  ~= k_2
        ~\wedge~  k_{ys1}  = k_2 
        ~\wedge~  k_{ys2}  = k_2
\end{array}
$$

To produce the final size scheme we lookup the sizes of the input and output variables of the original function from the solved constraints, and generalize appropriately. This process is determined by the top-level rule in Figure~\ref{f:ConstraintGeneration}. In this case no rigid size variables appear in the result, so we can universally quantify all size variables.
$$@normalize@ ~:_s \forall k. (xs : k) \to (ys_1 : k,~ ys_2 : k)
$$

\input{figures/Size-env.tex}


% -----------------------------------------------------------------------------
\subsection{Rigid Sizes}
When the environment of our size constraints contains rigid variables (indicate by $\exists k$), we introduce existential quantifiers instead of universal quantifiers into the size scheme. Consider the @filterLeft@ function from S~\ref{s:SizeTypes}
\begin{code}
      filterLeft xs
        = let ys1 = map (+ 1)   xs
              ys2 = filter even xs
          in (ys1, ys2)
\end{code}
The size constraints for this function, already in solved form, are as follows.
$$
\begin{array}{ll}
       & xs : k_{xs},~ ys_1 : k_{ys1},~ \exists k_1,~ ys_2 : k_{ys2},~ k_2
\\
\vdash &          k_{ys_1} = k_1
        ~\wedge~  k_{ys_2} = k_2
        ~\wedge~  k_{xs}   = k_2
\end{array}
$$

As variable $k_2$ is marked as rigid, we introduce an existential quantifier for it, producing the size scheme stated earlier:

\begin{alltt}
   filterLeft \(:\sb{s}\,\forall\,k\sb{1}.\,\exists\,k\sb{2}.\,(xs\,:\,k\sb{1})\;\to\;(ys\sb{1}\,:\,k\sb{1},\,ys\sb{2}\,:\,k\sb{2})\)
\end{alltt}

Note that although Rule~(SFun) from Figure~\ref{f:ConstraintGeneration} performs a \emph{generalisation} process, there is no corresponding instantiation rule. The size inference process works on the entire graph at a time, and there is no mechanism for one operator to invoke another. To say this another way, all subgraphs are fully inlined. Recall from \S\ref{s:CombinatorNormalForm} that we assume our operator graphs are embedded in a larger host program. We use size information to guide the clustering process, and although the host program can certainly call the operator graph, static size information does not flow across this boundary.

When producing size schemes, we do not permit the arguments of an operator graph to have existentially quantified sizes. This restriction is necessary to reject programs that we cannot statically guarantee will be well sized. For example:
\begin{code}
    bad1 xs  = let flt   = filter p xs
                   ys    = map2   f flt xs
               in  ys
\end{code}

The above function filters its input array, and then applies @map2@ to the filtered version, as well as the original array. As the @map2@ operators requires both of its arguments to have the same size, @bad1@ would only be valid when the predicate @p@ is always true. The size constraints are as follows:
$$
\begin{array}{ll}
       & xs : k_{xs},~ flt : k_{flt},~ \exists k_1,~ ys : k_{ys},~ k_2
\\
\vdash &          k_{flt}  = k_1
        ~\wedge~  k_{flt}  = k_2
        ~\wedge~  k_{xs}   = k_2
        ~\wedge~  k_{ys}   = k_2
\end{array}
$$

\noindent
Solving this then yields:
$$
\begin{array}{ll}
       & xs : k_{xs},~ flt : k_{flt},~ \exists k_1,~ ys : k_{ys},~ k_1
\\
\vdash &          k_{flt}  = k_1
        ~\wedge~  k_{xs}   = k_1
        ~\wedge~  k_{ys}   = k_1
\end{array}
$$

In this case Rule~(SFun) does not apply, because the parameter variable $xs$ has size $k_1$, but $k_1$ is marked as rigid in the environment (with $\exists k_1$). 

As a final example, the following function is ill-sized, because the two filter operators are not guaranteed to produce the same number of elements.
\begin{code}
     bad2 xs = let as  = filter p1 xs
                   bs  = filter p2 xs
                   ys  = map2   f  as bs
               in  ys
\end{code}

The initial size constraints for this function are:
$$
\begin{array}{ll}
       & xs : k_{xs},~ as : k_{as},~ \exists k_1,~ bs : k_{bs},~ \exists k_2,~ ys : k_{ys},~ k_3
\\
\vdash &          k_{as}   = k_1
        ~\wedge~  k_{bs}   = k_2
        ~\wedge~  k_{as}   = k_3
        ~\wedge~  k_{bs}   = k_3
        ~\wedge~  k_{ys}   = k_3
\end{array}
$$

To solve these, we note that $k_{as}$ is used twice on the left of an atomic equality constraint, so substitute $k_1$ for $k_3$:
$$
\begin{array}{ll}
       & xs : k_{xs},~ as : k_{as},~ \exists k_1,~ bs : k_{bs},~ \exists k_2,~ ys : k_{ys},~ k_1
\\
\vdash &          k_{as}   = k_1
        ~\wedge~  k_{bs}   = k_2
        ~\wedge~  k_{bs}   = k_1
        ~\wedge~  k_{ys}   = k_1
\end{array}
$$

At this stage we are stuck because the constraints are not yet in solved form, and we cannot simplify them further. Both $k_1$ and $k_2$ are marked as rigid, so we cannot substitute one for the other and produce a single atomic constraint for $k_{bs}$.


% \newcommand{\eqclasses}[1]{
%     \begin{tabbing}
%         MM \= M \= \kill
%         #1
%     \end{tabbing}}
% 
% \newcommand{\eqclass}[2]{$#1$ \> $\in$ \> $\{#2\}$ \\}

% As an example of constraint generation, here is the @normalize2@ program from earlier.
% \begin{tabbing}
% @MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM@  \= \kill
% @normalize2 us@                     \> $\exists k_{us}.$      \\
% @ = let sum1 = fold   (+) 0 us@     \>                      \\
% @       gts  = filter (>0)  us@     \> $\forall k_{gts}.$ \\
% @       sum2 = fold   (+) 0 gts@    \> \\
% @       nor1 = map  (/sum1) us@     \> $\exists k_{nor1}.\ k_{nor1} = k_{us} \wedge$ \\
% @       nor2 = map  (/sum2) us@     \> $\exists k_{nor2}.\ k_{nor2} = k_{us} \wedge$ \\
% @   in (nor1, nor2)@                \> $true$ \\
% \end{tabbing}
% This constraint is valid, with the equivalence classes being:
% \eqclasses{
%     \eqclass{k_{us}}{k_{us}, k_{nor1}, k_{nor2}}
%     \eqclass{k_{gts}}{k_{gts}}
% }

% The next example involves two filters using the same predicate.
% Despite using the same predicate and input data, we produce different output sizes for each filter.
% \begin{tabbing}
% @MMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMM@  \= \kill
% @diff xs@                           \> $\exists k_{xs}.$ \\
% @ = let ys1 = filter p xs@          \> $\forall k_{ys1}.$       \\
% @       ys2 = filter p xs@          \> $\forall k_{ys2}.$       \\
% @   in (ys1, ys2)@                  \> $true$                   \\
% \end{tabbing}
% 
% This constraint is valid, with the equivalence classes being:
% \eqclasses{
%     \eqclass{k_{xs}}    {k_{xs}}
%     \eqclass{k_{ys1}}   {k_{ys1}}
%     \eqclass{k_{ys2}}   {k_{ys2}}
% }




% -----------------------------------------------------------------------------
\subsection{Iteration size}
After the constraints are validated and equivalence classes generated, each combinator is assigned a $size$ as an iteration size -- the number of iterations in the loop required to generate the output.
It is important to note that for filters, the iteration size is not the output size, but is instead the size of the input.
The output size of a filter is, however, a \emph{subsize} of the input's size, as not only is it known to be less than or equal to its input in size, it is also generated depending on the input.
The iteration sizes, $\tau_n$, are used to check whether two loops may be fused.
Any two iteration sizes in the same equivalence class are the same size, and so are fusible.
The difference from previous work is that loops of different iteration sizes \emph{can} be fused, if one is a subsize of the other, and the subsize's \emph{generator} is fused together it as well.
Basically, operations on filtered data can be fused with operations on the original data, if it is fused with the filter as well.
External computations are treated separately, as they cannot be fused with any other nodes.

\begin{tabbing}
MMMMM       \= MM \= MMMMMMMMMMM \= \kill
$T$          \> @::=@ \> $\tau$                                  \> (known size) \\
             \> $~|$  \> @external@                              \> (external and unfusible) \\
\end{tabbing}

Once the constraints are solved, known to be valid, and sorted into equivalence classes, each combinator is assigned a size.
Note that for a filter, the size of the output array $k_o$ is some existential that is less than or equal to $k_n$, but the actual loop size of the \emph{combinator} is equal to $k_n$.
This is because, in order to produce the filtered output, all elements of the input $n$ must be considered.


\begin{tabbing}
MM \= MM \= MMMMMMMMM \= MMMM \= MM \= \kill
$\tau$  \>$::$\> $binds \rightarrow name \rightarrow T$ \\
\\
$\tau_{bs,o}$    
            \> $|$ \> $o = @fold@~f~n$      \> $\in bs$ \> $=$ \> $k_n$ \\
            \> $|$ \> $o = @map@_n~f~ns$    \> $\in bs$ \> $=$ \> $k_o$ \\
            \> $|$ \> $o = @filter@~f~n$    \> $\in bs$ \> $=$ \> $k_n$ \\
            \> $|$ \> $o = @generate@~s~f$  \> $\in bs$ \> $=$ \> $k_o$ \\
            \> $|$ \> $o = @gather@~i~d$    \> $\in bs$ \> $=$ \> $k_i$ \\
            \> $|$ \> $o = @cross@~a~b$     \> $\in bs$ \> $=$ \> $k_a \times k_b$ \\
            \> $|$ \> $o = @external@~ins$  \> $\in bs$ \> $=$ \> $@external@$ \\
\end{tabbing}

After the generated equality constraints are solved, and sizes are grouped into equivalence classes, combinators with iteration sizes in the same equivalence class may be fused together if there are no fusion-preventing dependencies between them.


% -----------------------------------------------------------------------------
\subsection{Transducers}
Unlike previous work, we do allow combinators with different iteration sizes to be fused together.
For example, an operation on filtered data may be fused with the filter operation that generates the data, even though the iteration sizes are different.

We define the concept of \emph{transducers} as combinators having a different output size to their iteration size.
As usual, a transducer may fuse with other nodes of the same iteration size, but transducers may also fuse with nodes having iteration size the same as the transducer's output size.
For our set of combinators, the only transducer is @filter@.

Looking back at the @normalize@ example, the iteration size of @gts@ and @sum1@ are both $k_{xs}$.
%The iteration size of @sum2@ is $k_{gts}$, and @gts@ is a transducer from $k_{xs}$ to $k_{gts}$. 
%\gabi{gts is no combinator, so can't be a transducer
The iteration size of @sum2@ is $k_{gts}$, and the filter combinator which produces @gts@ is a transducer from $k_{xs}$ to $k_{gts}$. 
Despite it not being known whether $k_{gts} = k_{xs}$, the three nodes @gts@, @sum1@ and @sum2@ can all be fused together.
The two nodes @sum2@ and @gts@ can be fused together, since the output size of @gts@ is the iteration size of @sum2@.
Similarly, @sum1@ and @gts@ can be fused together, as they have the same iteration size.
\begin{code}
 normalize2 xs
  = let sum1 = fold   (+)  0   xs
        gts  = filter (> 0)    xs
        sum2 = fold   (+)  0   gts
        ...
\end{code}

% \ben{Do other systems use the concept of transducers?} \TODO{Explain that a transducer of filter is the filter's parent. so $trans(trans(n)) \not= n$}
% No,
We now define a function $trans$, to find the parent transducer of a given combinator.
The parent transducer $trans(bs, n)$ of a combinator has the same output size as $n$'s iteration size, but the two have different iteration sizes.

\begin{tabbing}
MMMM \= MM \= MMMMMMMMM \= MMMM \= MM \= \kill
$trans$  \>$::$\> $binds \rightarrow name \rightarrow \{name\}$ \\
$trans(bs,o)$    \\
            \> $|$ \> $o = @filter@~f~n$    \> $\in bs$ \> $=$ \> $trans'(bs,n)$ \\
            \> $|$ \> otherwise             \>          \> $=$ \> $trans'(bs,o)$ \\
\\
$trans'(bs,o)$    \\
            \> $|$ \> $o = @fold@~f~n$      \> $\in bs$ \> $=$ \> $\emptyset$ \\
            \> $|$ \> $o = @map@_n~f~ns$    \> $\in bs$ \> $=$ \> $\bigcup_{x \in ns} trans(bs, x)$ \\
            \> $|$ \> $o = @filter@~f~n$    \> $\in bs$ \> $=$ \> $\{o\}$       \\
            \> $|$ \> $o = @generate@~s~f$  \> $\in bs$ \> $=$ \> $\emptyset$ \\
            \> $|$ \> $o = @gather@~i~d$    \> $\in bs$ \> $=$ \> $trans(bs,i)$ \\
            \> $|$ \> $o = @cross@~a~b$     \> $\in bs$ \> $=$ \> $\emptyset$ \\
            \> $|$ \> $o = @external@~ins$  \> $\in bs$ \> $=$ \> $\emptyset$ \\
\end{tabbing}

To determine whether two combinators of different iteration sizes may be fused together, we first find parent or ancestor transducers of the same size, if they exist:
\begin{tabbing}
MMMM \= M \= MMMMMMM \= M \= \kill
$parents$ \> $::$ \> $name \to name \to \{name \times name\}$ \\
$parents(a,b)$ \\
        \> $|$ \> $\tau(a) == \tau(b)$ \> $=$    \> $\{(a, b)\}$ \\
        \> $|$ \> otherwise            \> $=$    \> $\{ parents(a', b) ~|~ a' \in trans(a) \} $      \\
        \>     \>                      \> $\cup$ \> $\{ parents(a, b') ~|~ b' \in trans(b) \} $  \\
\end{tabbing}

\textbf{Lemma: sole transducers}.
For some bindings $bs$, each name $n$ will have at most one transducer.
\[
valid(\constr{bs}) \implies \forall n.\ |trans(bs, n)| \le 1
\]
% \textbf{Proof:} by induction on $bs$. If $n = @map@_n~f~ns$,
% then $trans(bs,n) = \bigcup_{x \in ns} trans(bs,x)$.
% As @map@ requires its arguments to have the same size, and @filter@ introduces a fresh size for its output,
% if any of the $trans(bs,x)$ are non-empty, they will refer to the same @filter@.
% The other cases are trivial.

\textbf{Lemma: transducers change types}.
For some bindings $bs$, if $n$ has a transducer, the iteration size is not the same.
\[
valid(\constr{bs}) \wedge \{m\} = trans(bs, n) \implies \tau(n) \not= \tau(m)
\]

\textbf{Lemma: sole parents}.
For some bindings $bs$, each pair of names $a$ and $b$ will have at most one set of parents $parents(a,b)$.
\[
valid(\constr{bs}) \implies \forall a,b.\ |parents(bs, n)| \le 1
\]
% \textbf{Proof:} since each binding only has at most one transducer, and transducers' iteration sizes are not the same, there will be at most one pair of transducers up the chain that have equal types.

