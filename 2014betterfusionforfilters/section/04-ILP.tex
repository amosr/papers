%!TEX root = ../Main.tex

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{figures/ex2-normalizeInc.pdf}
\end{center}
\caption{Possible clusterings for \texttt{normalizeInc}}
\label{f:normalizeInc}
\end{figure}


% -----------------------------------------------------------------------------
\section{Integer Linear Programming}
\label{s:ILP}
It is usually possible to cluster a program graph in multiple ways. For example, consider the following simple function:
\begin{code}
 normalizeInc :: Array Int -> Array Int
 normalizeInc xs
  = let incs = map  (+1)    us
        sum1 = fold (+) 0   us
        ys   = map  (/ sum) incs
    in  ys
\end{code}

Two possible clusterings are shown in Figure~\ref{f:normalizeInc}. One option is to compute @sum1@ first and fuse the computation of @incs@ and @ys@. Another option is to fuse the computation of @incs@ and @sum1@ into a single loop, then compute @ys@ separately. A third option (not shown) is to compute all results separately, and not perform any fusion. 

Which option is better? On current hardware we generally expect the cost of memory access to dominate runtime. The first clustering in Figure~\ref{f:normalizeInc} requires two reads from array @xs@ and one write to array @ys@. The second requires a single fused read from @xs@, one write to @incs@, a read back from @incs@ and a final write to @ys@. From the size constraints of the program we know that all intermediate arrays have the same size, so we expect the first clustering will peform better as it only needs three array accesses instead of four. 

For small programs such as @normalizeInc@ it is possible to naively enumerate all possible clusterings, select just those that are \emph{valid} with respect to fusion preventing edges, and chose the one that maximises a cost metric such as the number of array accesses needed. However, as the program size increases the number of possible clusterings becomes too large to naively enumerate. For example, Pouchet et al~\cite{pouchet2010combined} present a fusion system using the polyhedral model~\cite{pouchet2011polyhedral} and report that some simple numeric programs have over 40,000 possible clusterings, with one particular example having $10^{12}$. 

To deal with the combinatorial explosion in the number of potential clusterings, we instead use an Integer Linear Programming (ILP) formulation. ILP problems are defined as a set of variables, an objective linear function and a set of linear constraints. The integer linear solver finds an assignment to the variables that minimises the objective function, while satisfying all constraints. For the clustering problem we express our constraints regarding fusion preventing edges as linear constraints on the ILP variables, then use the objective function to encode our cost metric. This general approach was first fully described by Megiddo and Sarkar~\cite{megiddo1998optimal}, and our main contribution is to extend it to work with size changing operators such as @filter@. 



% -----------------------------------------------------------------------------
\subsection{Dependency Graphs}
A dependency graph represents the data dependencies of the program to be fused, and we use it as an intermediate stage when producing linear constraints for the ILP problem. The dependency graph contains enough information to determine the possible clusterings of the input program, while abstracting away from the exact operators used to compute each intermediate array. The rules for producing a dependency graphs are in Figure~\ref{f:DependencyGraph}.

Each binding in the source program becomes a node in the dependency graph. For each intermediate variable, we add a directed edge from the binding that produces a value to all bindings that consume it. Each edge is also marked as either \emph{fusible} or \emph{fusion preventing}. Fusion preventing edges are used when the producer must finish its execution before the consumer node can start. For example, a @fold@ operation must complete execution before it can produce the scalar value needed by its consumers. Conversely, the @map@ operation produces an output value for each value it consumes, so is marked as fusible. 

The @gather@ operation is a hybrid: it takes an indices array and an elements array, and for each element in the indices array returns the corresponding data element. This means that gather can be fused with the operation that produces its indices, but not the operation that produces its elements --- because those are accessed in a random-access manner. 

% Each node may simply be the name of its output binding (or bindings, in the case of @external@); as we require names to only be bound once, this is assured to be unique. Creating edges between these nodes is simply when one binding references an earlier one. The only complication is designating edges as \emph{fusible} or \emph{fusion-preventing}.


\begin{figure}
\begin{tabbing}
MMMMM       \= M  \= \kill
$nodes$     \> @:@ \> $program \to V$              \\
$edges$     \> @:@ \> $program \to E$              \\
$edge$      \> @:@ \> $\{bind\} \times bind \to E$ 
\\[1ex]
$nodes(bs)$ \> $= \{(name(b), \iiter_{\Gamma,C}(b)) | b \in bs\}$       
\\[2ex]

$edges(bs)$ \> $= \bigcup_{b \in bs}edge(bs, b)$    
\\[1ex]
MM             \= M \= \kill
$edge(bs, out = @fold@~f~in)$ \\
    \> $=$    \> $\{inedge(bs,out,s) | s \in fv(f)\} \cup \{inedge(bs, out, in) \}$       \\
$edge(bs, out = @map@~f~in)$  \\
    \> $=$    \> $\{inedge(bs,out,s) | s \in fv(f)\} \cup \{inedge(bs, out, in) \}$       \\
$edge(bs, out = @filter@~f~in)$  \\
    \> $=$    \> $\{inedge(bs,out,s) | s \in fv(f)\} \cup \{inedge(bs, out, in) \}$       \\
$edge(bs, out = @gather@~data~indices)$  \\
    \> $=$    \> $\{(out,data, \fusionpreventing) \} \cup \{inedge(bs, out, indices) \}$       \\
$edge(bs, out = @cross@~a~b)$            \\
    \> $=$    \> $\{inedge(bs, out, a) \}           \cup      \{(out, b, \fusionpreventing) \}$ \\
$edge(bs, outs = @external@~ins)$  \\
    \> $=$    \> $\{(outs,i, \fusionpreventing) | i \in ins \}$ 
\\[1ex]
$inedge(bs,to,from)$ \\
    \> $|$ \> $(from = @fold@~f~s) \in bs$     \\
    \> $=$ \> $(to, from, \fusionpreventing)$  \\
    \> $|$ \> $(outs = @external@ \ldots) \in bs     \wedge from \in outs$     \\
    \> $=$ \> $(to, outs, \fusionpreventing)$  \\
    \> $|$ \> $otherwise$                      \\
    \> $=$ \> $(to, from, \fusible)$
\end{tabbing}

\caption{Dependency Graphs from Programs}
\label{f:DependencyGraph}
\end{figure}


% -----------------------------------------------------------------------------
\subsection{Graph $\to$ ILP}
With the dependency graph now constructed, it must be converted to an integer linear program. The integer linear program will contain some variables, an objective function, and constraints that the variables must conform to. An external solver will find and return a variable assignment that minimises the objective function. With the variable assignment, the graph's nodes are partitioned into a clustering, and then flow fusion\cite{lippmeier2013flow} is used to extract imperative loops for each cluster.

The variables in the ILP formulation can be split into three groups:

\begin{tabbing}
M   \= MM \= MMMMMMM \= MM \= \kill
$x$   \> @:@  \> $node \times node$ \> $\to$ \> $\mathbb{B}$
\end{tabbing}
The first and most important group is created for every pair of nodes, and has a boolean value indicating whether the two nodes can be fused. If $x_{ij} = 0$, then $i$ and $j$ are fused together. While it may seem counterintuitive for 0 to mean fused and 1 unfused, it means the objective function can simply minimise a weighted $x_{ij}$, and also makes later precedence constraints simpler. The values of these variables are used to construct the partitioning at the end, such that $\forall i,j.\ x_{ij} = 0 \iff cluster(i) = cluster(j)$.
If two nodes are ``fused together'', it means that their combinators will be in the same cluster.

\begin{tabbing}
M   \= MM \= MMMMMMM \= MM \= \kill
$\pi$ \> @:@  \> $node$             \> $\to$ \> $\mathbb{R}$
\end{tabbing}
The second group of variables is used to ensure that the clustering is acyclic -- that is, there must be an ordering to execute each clusters' dependency clusters before executing the cluster itself.
For each node $i$, we associate a real $\pi_i$ such that every node $j$ after $i$ we have $\pi_j > \pi_i$. This has the effect of requiring an acyclic clustering, as if one node is after another its $\pi$ must be strictly greater than its predecessor, and the predecessor must be strictly less than the successor. If two nodes are to be fused together, their $\pi$ values must be the same.

As an example of a cyclic clustering, consider:
\begin{code}
cycle us
 = let xs  = map (+1) us          (C1)
       sum = fold xs              (C2)
       zs  = map (+sum) xs        (C1)
\end{code}
There is no fusion-preventing edge directly between @xs@ and @zs@, but there is a fusion-preventing edge between @sum@ and @zs@.
If @xs@ and @zs@ were clustered together in @C1@ and @sum@ were in cluster @C2@, there would be a dependency cycle between @C1@ and @C2@, and neither could be executed before the other.

\begin{tabbing}
M   \= MM \= MMMMMMM \= MM \= \kill
$c$   \> @:@  \> $node$             \> $\to$ \> $\mathbb{B}$
\end{tabbing}
The third, and final, group of variables are purely for minimisation, indicating whether a node's array is \emph{fully contracted}.
They are not required for correctness of the clustering, but are used in the objective function.
A node's array will be fully contracted --- that is, it will be replaced by a scalar --- if all nodes that use this array are fused together: $c_i = 0 \iff \forall (i',j) \in E.\ i = i' \implies x_{ij} = 0$.


% -----------------------------------------------------------------------------
\subsubsection{Unoptimised version}
Before showing the optimised version with certain constraints removed (\S\ref{s:OptimisedConstraints}), this simpler, unoptimised version is shown.
The only difference is that fewer constraints and variables are required in the optimised version, but both versions give the same clustering.
This unoptimised version generates clustering constraints and variables for every pair of nodes, regardless of whether they may, in fact, be fused together.
Later, we show that certain constraints and variables can be removed when there is a fusion-preventing edge between two nodes.


\paragraph{Acyclic and precedence-preserving}

\begin{tabbing}
MMMMM   \= MMM \= M \= MMM \= M \= MMM \= \kill
Minimise   \> \ldots \\
Subject to \> \ldots \\
           \>    $x_{ij}$ \> $\le$ \> $\pi_j - \pi_i$ \> $\le$ \> $N x_{ij}$ \\
           \>             (an edge from $i$ to $j$)            \\
           \> $-N x_{ij}$ \> $\le$ \> $\pi_j - \pi_i$ \> $\le$ \> $N x_{ij}$ \\
           \>             (no edge from $i$ to $j$)            \\
           \> \ldots
\end{tabbing}
As per Megiddo\cite{megiddo1998optimal}, we look at every pair of nodes $i$ and $j$.

Whether there is an edge between $i$ and $j$ or not, if the two are fused together, then $x_{ij} = 0$. If $x_{ij} = 0$, then both cases may be simplified to $0 \le \pi_j - \pi_i \le 0$, which is equivalent to $\pi_i = \pi_j$. \ben{Highlight that ``fused together'' means ``in the same cluster'', and that operators can be fused even if there are no direct edges between them.}

Otherwise, if the two nodes are not fused together, then $x_{ij} = 1$. If there is an edge between $i$ and $j$, the constraint simplifies to $1 \le \pi_j - \pi_i \le N$, where $N$ is the number of nodes in the graph. This constraint means that the difference between the two $\pi$s must be at least 1, and less than $N$. Since there are $N$ nodes, the maximum difference between any two $\pi$s would be at most $N$, so the upper bound of $N$ is large enough to be safely ignored. This means the constraint can roughly be translated to $\pi_i < \pi_j$, which enforces the acyclic constraint.

If there is no edge between $i$ and $j$ and the two are not fused together, then $x_{ij} = 1$ and the constraint simplifies to $-N \le \pi_j - \pi_i \le N$, which effectively puts no constraint on the $\pi$ values.
Note, however, that other edges may still constrain $i$ or $j$.


\paragraph{Fusion-preventing edges}
\begin{tabbing}
MMMMM   \= MMM \= M \= MMM \= M \= MMM \= \kill
Minimise   \> \ldots \\
Subject to \> \ldots \\
           \> $x_{ij}$    \> $=$   \> $1$             \>       \>            \\
           \> (fusion-preventing edges from $i$ to $j$)      \\
           \> \ldots
\end{tabbing}
As per Megiddo\cite{megiddo1998optimal}, for every fusion-preventing edge, we add the constraint $x_{ij} = 1$, so that no fusion can occur. This, together with the precedence constraints above, has the effect of enforcing $\pi_i < \pi_j$.


\paragraph{Fusion between different iteration sizes}

\begin{tabbing}
MMMMM   \= MMM \= M \= MMM \= M \= MMM \= \kill
Minimise   \> \ldots \\
Subject to \> \ldots \\
           \> $x_{ij}$    \> $=$   \> $1$             \>       \>            \\
           \> (if $\iiter_{\Gamma,C}(i) \not= \iiter_{\Gamma,C}(j) \wedge parents(i,j) = \emptyset$)  \\
           \\
           \> $x_{ij}$    \> $=$   \> $1$             \>       \>            \\
           \> (if $\bot \in \{\iiter_{\Gamma,C}(i), \iiter_{\Gamma,C}(j)\}$)  \\
           \\
           \> $x_{i'j}$   \> $\le$ \> $x_{ij}$        \>       \>            \\
           \> $x_{ij'}$   \> $\le$ \> $x_{ij}$        \>       \>            \\
           \> $x_{i'j'}$   \> $\le$ \> $x_{ij}$        \>       \>            \\
           \> (if $\iiter_{\Gamma,C}(i) \not= \iiter_{\Gamma,C}(j) \wedge parents(i,j) = \{i',j'\}$) \\
           \> \ldots
\end{tabbing}
If two nodes have different iteration sizes, they may not necessarily be fused.
If either of the nodes have no iteration size --- in which case, they are @external@ computations --- they may not be fused at all.
However, if they have parent transducers of the same size, they may be fused if both their parents are fused together, and each is fused with its respective parent.

This is the main difference to existing integer linear programming solutions: we allow nodes with different iteration sizes to be fused together, when their parent transducers are fused together.
This is an important feature for combinator-based programming, where size-changing operations such as @filter@ are used frequently.
\ben{Highlight why we want to allow this sort of fusion, and why it is specific to data flow fusion.}

As a degenerate example, consider fusing an operation on filtered data with its generating filter:
\begin{code}
 sum1 = fold (+) 0  us
 gts  = filter (>0) us
 sum2 = fold (+) 0  gts
\end{code}
Here, $parents(gts,sum2) = \{gts, gts\}$.
This generates spurious, but still valid, constraints that $gts$ must be fused with $gts$ and $gts$ must be fused with $sum2$, in order for $gts$ and $sum2$ to be fused together.
While these constraints are unnecessary in this case, they are harmless.

In the same example, $sum1$ and $sum2$ also have different iteration sizes.
We have that $parents(sum1,sum2) = \{sum1, gts\}$.
This means that $sum1$ and $sum2$ may only be fused if $sum1$ is fused with $sum1$ (trivial), $sum2$ is fused with $gts$, and $sum1$ is fused with $gts$.
While it may seem like these constraints should be implied by transitivity of clustering, all clustering variables are used for the objective function.


\paragraph{Array contraction}
\begin{tabbing}
MMMMM   \= MMM \= M \= MMM \= M \= MMM \= \kill
Minimise   \> \ldots \\
Subject to \> \ldots \\
           \> $x_{ij}$    \> $\le$ \> $c_i$           \>       \>            \\
           \> (for all edges from i)            \\
           \> \ldots \\
\end{tabbing}
As per Darte's work on optimising for array contraction\cite{darte2002contraction}, we define a variable $c_i$ for each array.
An array is fully contracted if all its output edges are fused with it.
Thus, $c_i=0$ only if $\forall (i',j) \in E.\ i = i' \implies x_{ij} = 0$.
By minimising $c_i$ in the objective function, we favour solutions that reduce the number of required intermediate arrays.

If an array is fully contracted, it means that the array does not need to be materialised in memory, and can simply be replaced by a scalar variable for the current element. Contracting arrays reduces memory traffic and can reduce overall memory usage.


% -----------------------------------------------------------------------------
\subsubsection{The objective function}
\label{s:ObjectiveFunction}

To find the objective function, note that fusing loops can have three main benefits:
\begin{itemize}
\item
reducing memory traffic, such as multiple loops reading from the same array;
\item
removing intermediate arrays, thus reducing the amount of memory required;
\item
and reducing loop overhead, such as when two loops operate on different arrays of the same size.
\end{itemize}
However, these benefits cannot be considered in isolation; for example, fusing two loops to reduce loop overhead may remove potential fusion opportunities that reduce memory traffic.
When operating on large arrays that do not fit in cache, memory traffic dominates execution time.
An excessive number of intermediate arrays can also cause issues if all are live in memory at once, potentially leading to \emph{thrashing}.
The benefits of removing loop overhead are least of all; it should be performed if possible, but must never remove opportunities for other fusion. \TODO{Refer to example code that demonstrates these different opportunities.}

This total ordering can be encoded into an ILP objective function as weights.
If the program graph contains $N$ combinators, then there are at most $N$ opportunities for fusion.
The encoding of loop overhead is weight $1$, removing intermediate arrays is weight $N$, and reducing memory traffic is weight $N^2$.
This ensures that no amount of loop overhead reduction can outweigh the benefit of removing an intermediate array,
and likewise no number of removed intermediate arrays can outweigh a reduction in memory traffic.
Although, it is worth noting that reducing memory traffic does \emph{tend} to remove intermediate arrays, and vice versa.

\begin{tabbing}
MMMMM   \= MMMM \= M \= \kill
Minimise   \>     \> $\Sigma_{(i,j) \in E} W_{ij} x_{ij}$   \\
           \> \> \> (memory traffic and loop overhead)         \\
           \> $+$ \> $\Sigma_{i \in V} N c_i$  \\
           \> \> \> (removing intermediate arrays)         \\
Subject to \> \ldots                                \\
Where      \>                                       \\
           \> $W_{ij} = N^2$ \> $~|$ \> $(i,j) \in E $         \\
           \> \> \> (fusing $i$ and $j$ will reduce memory traffic)         \\
           \> $W_{ij} = N^2$ \> $~|$ \> $\exists k. (k,i) \in E \wedge (k,j) \in E $     \\
           \> \> \> ($i$ and $j$ share an input array)                                         \\
           \> $W_{ij} = 1$   \> $~|$ \> $@otherwise@$                                                  \\
           \> \> \> (the only benefit is loop overhead)                                        \\
           \\
           \> $N = |V|$
\end{tabbing}


\subsubsection{A note on transitivity}
It may seem that we could generate far fewer constraints and rely on transitivity of clustering equality $x_{ij}$.
This means that for each pair $i,j$ we wouldn't generate an $x_{ij}$ and thus not generate those constraints, unless $i$ or $j$ is a @filter@, or arc between $i$ and $j$.
This would help, but also means we can't \emph{minimise} on these removed $x_{ij}$, so the solution won't count the (rather smaller) benefits of fusing two non-connected nodes.

\subsubsection{Fusion-preventing path optimisation}
\label{s:OptimisedConstraints}
We can remove certain constraints and variables, by only checking nodes if there is no \emph{fusion-preventing path} between $i$ and $j$.
If there is a fusion-preventing path between two nodes, they are known to be in different clusters.
With our specific knowledge of the problem domain, we can simplify the integer linear program and make the job of the ILP solver easier by omitting these constraints.
Creating fewer constraints and variables should make it faster for the ILP solver to find a solution.

For the graph, we define a function $split(a,b)$ to check whether there exists a fusion-preventing path between two nodes, $a$ and $b$.

\begin{tabbing}
MMM      \= MM   \=  MMMMMMMMMMMM    \=  \kill
$split$     \> @::@ \> $name \times name \to \mathbb{B}$      \\
MM        \= M    \= \kill
$split(a,b)$ \\
    \>$=$  \>$\forall p \in path(a,b) \cup path(b,a).\ \fusionpreventing \not\in p$\\
\end{tabbing}

With $split$ defined, we can refine our formulation to only generate constraints between two nodes if there is a chance they may be fused together.
The entire formulation of the integer linear program follows.

\begin{tabbing}
MMMMM   \= MMM \= M \= MMM \= M \= MMM \= \kill
Minimise   \> $\Sigma_{(i,j) \in E} W_{ij} x_{ij} + \Sigma_{i \in V} N c_i$  \\
           \> (if $split(i,j)$)         \\
Subject to \\
           \> $-N x_{ij}$ \> $\le$ \> $\pi_j - \pi_i$ \> $\le$ \> $N x_{ij}$ \\
           \> (if $split(i,j) \wedge (i,j) \not\in E \wedge (j,i) \not\in E$)            \\
\\
           \>    $x_{ij}$ \> $\le$ \> $\pi_j - \pi_i$ \> $\le$ \> $N x_{ij}$ \\
           \> (if $split(i,j) \wedge (i,j,\fusible) \in E$)     \\
\\
           \>             \>       \> $\pi_i < \pi_j$ \>       \>            \\
           \> (if $(i,j,\fusionpreventing) \in E$)    \\
\\
\\
           \> $x_{ij}$    \> $\le$ \> $c_i$           \>       \>            \\
           \> (if $split(i,j)$) \\
           \> $c_{i }$    \> $ = $ \> $ 1 $           \>       \>            \\
           \> (if $\neg(split(i,j))$) \\
\\
           \> $x_{ij}$    \> $=$   \> $1$             \>       \>            \\
           \> (if $\iiter_{\Gamma,C}(i) \not= \iiter_{\Gamma,C}(j) \wedge parents(i,j) = \emptyset$)  \\
           \\
           \> $x_{ij}$    \> $=$   \> $1$             \>       \>            \\
           \> (if $\bot \in \{\iiter_{\Gamma,C}(i), \iiter_{\Gamma,C}(j)\}$)  \\
           \\
           \> $x_{i'j}$   \> $\le$ \> $x_{ij}$        \>       \>            \\
           \> $x_{ij'}$   \> $\le$ \> $x_{ij}$        \>       \>            \\
           \> $x_{i'j'}$   \> $\le$ \> $x_{ij}$        \>       \>            \\
           \> (if $\iiter_{\Gamma,C}(i) \not= \iiter_{\Gamma,C}(j) \wedge parents(i,j) = \{i',j'\} \wedge$ \\

           \> \> $ split(i,j) \wedge split(i',j) \wedge split(i,j') \wedge split(i',j') $) \\
MMMMM   \= MMMM \= M \= \kill
Where      \>                                       \\
           \> $W_{ij} = N^2$ \> $~|$ \> $(i,j) \in E $         \\
           \> \> \> (fusing $i$ and $j$ will reduce memory traffic)         \\
           \> $W_{ij} = N^2$ \> $~|$ \> $\exists k. (k,i) \in E \wedge (k,j) \in E $     \\
           \> \> \> ($i$ and $j$ share an input array)                                         \\
           \> $W_{ij} = 1$   \> $~|$ \> $@otherwise@$                                                  \\
           \> \> \> (the only benefit is loop overhead)                                        \\
           \\
           \> $N = |V|$
\end{tabbing}

% \input{section/04-ILP-Proof.tex}
