\section{Integer linear program formulation}
While the different combinators are all implemented differently, certain aspects are shared between all; they operate on input arrays, use scalar variables in their worker functions, and produce output.
To simplify creation of the integer linear program formulation, we first convert a function to a dependency graph.
The dependency graph has a node for each combinator, and edges between two combinators when one uses another's output.
After the graph is created, it is converted to an integer linear program with an objective of the least memory traffic and loops, solved, and the solution converted to a clustering.

When creating the graph, it is important to note that not all loops can be fused together; firstly, loops of different size cannot be fused.
Secondly, two loops cannot be fused together if an iteration in one loop relies on the output of a later iteration in another loop.
For example, a @map@ may not be fused with a @fold@ if the @map@'s worker function uses the result of the @fold@.
This fusion restriction is encoded as a \emph{fusion-preventing} edge between two combinators.
Other edges are \emph{fusible}, and may be fused together.
If they are not fused together for whatever reason, the \emph{from} combinator must be scheduled before the \emph{to} combinator.

The dependency graph is translated to an integer linear program.
The integer linear program has some integral, boolean and real variables, an objective function to minimise, and a set of constraints that the variables must conform to.
Finding a variable assignment that satisfies the constraints and is the minimal objective function is NP-hard, but existing ILP solvers tend to be sufficient for realistic problem sizes.
For larger problems, we can find an approximate answer, within say $10\%$ of the optimal answer, which still gives the exact answer for small problems.

\subsection{Function $\to$ graph}
Converting a function in \emph{combinator normal form} to a graph --- in fact, a DAG --- is quite simple.
Each binding becomes a node in the graph.
When a binding references other arrays or scalars, there is an edge between those two nodes.
Edges may be either \emph{fusible} or \emph{fusion-preventing}.
Fusion-preventing edges mean that the entire input node must completely finish its execution before the output node can start executing.
For example, @fold@s consume their entire input array before producing a single result, so any references to folds must be fusion-preventing.
Conversely, @map@s produce output data for every input element, so may be fused.

The @gather@ operation is interesting; it takes an indices array and a data array, and for each element in indices returns that element in data.
Thus, @gather@ requires random-access in its data array, and is not fusible, but consumes indices linearly and may fuse.

\begin{tabbing}
MMMM        \= MM   \= \kill
$nodes$     \> @::@ \> $program \to V$          \\
$edges$     \> @::@ \> $program \to E$          \\
$edge$      \> @::@ \> $\{bind\} \times bind \to E$\\
\\
MMMMM       \= \kill
$nodes(bs)$ \> $= \{(name(b), \tau_b) | b \in bs\}$       \\
\end{tabbing}
Each node may simply be the name of its output binding (or bindings, in the case of @external@); as we require names to only be bound once, this is assured to be unique.
Creating edges between these nodes is simply when one binding references an earlier one. The only complication is designating edges as \emph{fusible} or \emph{fusion-preventing}.

\begin{tabbing}
MMMMM       \= \kill
$edges(bs)$ \> $= \bigcup_{b \in bs}edge(b)$    \\
\\
MMMM             \= M \= \kill
$edge(bs, out = @fold@~f~in)$ \\
                              \> $=$    \> $\{inedge(bs,out,s) | s \in f\}$ \\
                              \> $\cup$ \> $\{inedge(bs, out, in) \}$       \\
$edge(bs, out = @map@~f~in)$  \\
                              \> $=$    \> $\{inedge(bs,out,s) | s \in f\}$ \\
                              \> $\cup$ \> $\{inedge(bs, out, in) \}$       \\
$edge(bs, out = @filter@~f~in)$  \\
                              \> $=$    \> $\{inedge(bs,out,s) | s \in f\}$ \\
                              \> $\cup$ \> $\{inedge(bs, out, in) \}$       \\
$edge(bs, out = @gather@~data~indices)$  \\
                              \> $=$    \> $\{(out,data, fusion~preventing) \}$ \\
                              \> $\cup$ \> $\{inedge(bs, out, indices) \}$       \\
$edge(bs, out = @cross@~a~b)$            \\
                              \> $=$    \> $\{inedge(bs, out, a) \}$       \\
                              \> $\cup$ \> $\{(out, b, fusion~preventing) \}$ \\
$edge(bs, outs = @external@~ins)$  \\
                              \> $=$    \> $\{(outs,i, fusion~preventing) | i \in ins \}$ \\
\\
$inedge(bs,to,from)$ \\
                     \> $|$ \> $(from = @fold@~f~s) \in bs$     \\
                     \> $=$ \> $(to, from, fusion~preventing)$  \\
                     \> $|$ \> $(\ldots,from,\ldots = @external@ \ldots) \in bs$     \\
                     \> $=$ \> $(to, from, fusion~preventing)$  \\
                     \> $|$ \> $otherwise$                      \\
                     \> $=$ \> $(to, from, fusible)$            \\
\end{tabbing}

\subsection{Graph $\to$ ILP}
With the dependency graph now constructed, it must be converted to an integer linear program.
The integer linear program will contain some variables, an objective function, and constraints that the variables must conform to.
An external solver will find and return a variable assignment that minimises the objective function.
With the variable assignment, the graph's nodes are partitioned into a clustering, and then flow fusion\cite{lippmeier2013flow} is used to extract imperative loops for each cluster.

The variables in the ILP formulation can be split into three groups:

\begin{tabbing}
M   \= MM \= MMMMMMM \= MM \= \kill
$x$   \> @::@  \> $node \times node$ \> $\to$ \> $\mathbb{B}$
\end{tabbing}
The first and most important group is created for every pair of nodes, and has a boolean value indicating whether the two nodes can be fused.
If $x_{ij} = 0$, then $i$ and $j$ are fused together.
While it may seem counterintuitive for 0 to mean fused and 1 unfused, it means the objective function can simply minimise a weighted $x_{ij}$, and also makes later precedence constraints simpler.
The values of these variables are used to construct the partitioning at the end, such that $\forall i,j.\ x_{ij} = 0 \iff cluster(i) = cluster(j)$.

\begin{tabbing}
M   \= MM \= MMMMMMM \= MM \= \kill
$\pi$ \> @::@  \> $node$             \> $\to$ \> $\mathbb{R}$
\end{tabbing}
The second group of variables is used simply to ensure that the resulting clustering is acyclic.
For some node $i$, we associate a real $\pi_i$ such that every node $j$ after $i$ has a value $\pi_j > \pi_i$.
This has the effect of requiring an acyclic clustering, as if one node is after another its $\pi$ must be strictly greater than its predecessor, and the predecessor must be strictly less than the successor.
If two nodes are to be fused together, their $\pi$ values must be the same.

\begin{tabbing}
M   \= MM \= MMMMMMM \= MM \= \kill
$c$   \> @::@  \> $node$             \> $\to$ \> $\mathbb{B}$
\end{tabbing}
The third, and final, group of variables are purely for minimisation, indicating whether a node's array is \emph{fully contracted}.
They are not required for correctness of the clustering, but are used in the objective function.
A node's array will be fully contracted --- that is, it will be replaced by a scalar --- if all nodes that use this array are fused together:
$c_i = 0 \iff \forall (i,j) \in E. x_{ij} = 0$.





\subsubsection{The simple version}
\TODO{Explain this simple version}
\begin{tabbing}
MMMMM   \= MMM \= M \= MMM \= M \= MMM \= \kill
Minimise   \> \ldots \\
Subject to \> $-N x_{ij}$ \> $\le$ \> $\pi_j - \pi_i$ \> $\le$ \> $N x_{ij}$ \\
           \>             (no edge from i to j, but there is a fusion benefit)            \\
           \>    $x_{ij}$ \> $\le$ \> $\pi_j - \pi_i$ \> $\le$ \> $N x_{ij}$ \\
           \>             (an edge from i to j)            \\
\\
           \> $x_{ij}$    \> $\le$ \> $c_i$           \>       \>            \\
           \> (for all edges from i; thus $c_i=0$ only if $\forall j | (i,j) \in E. x_{ij} = 0$)      \\
\\
           \> $x_{ij}$    \> $=$   \> $1$             \>       \>            \\
           \> (fusion-preventing edges from i to j)      \\
\\
           \> $x_{ij}$    \> $=$   \> $1$             \>       \>            \\
           \> (if $\tau_i \not= \tau_j \wedge gen_i=\emptyset \wedge gen_j=\emptyset$)                 \\
           \> $x_{i'j}$   \> $\le$ \> $x_{ij}$        \>       \>            \\
           \> (if $\tau_i \not= \tau_j \wedge gen_i=\{i'\}$) \\
           \> $x_{ij'}$   \> $\le$ \> $x_{ij}$        \>       \>            \\
           \> (if $\tau_i \not= \tau_j \wedge gen_j=\{j'\}$) \\
\end{tabbing}
It is worth noting that for the edge cases, $x_{ij} = 1 \implies \pi_i < \pi_j$, meaning if $i$ and $j$ are not fused together, $j$ must be scheduled after $i$.


\subsubsection{A slightly smarter version}
We can simplify the version above and remove many constraints and variables by noting that if there is a fusion-preventing edge between $i$ and $j$, $x_{ij} = 1$, so the precedence constraint can be simplified to just $\pi_i < \pi_j$, and the $x_{ij}$ variable removed.
Likewise with clusters of different, incompatible types.
The contraction variable $c_i$ can be removed, if $i$ has any fusion-preventing output edges: any such edges use the array and cannot be fused, thus there is no possibility of the array being contracted away.

\begin{tabbing}
MMMMM   \= MMM \= M \= MMM \= M \= MMM \= \kill
Minimise   \> \ldots \\
Subject to \> $-N x_{ij}$ \> $\le$ \> $\pi_j - \pi_i$ \> $\le$ \> $N x_{ij}$ \\
           \>             (no edge from i to j, but there is a fusion benefit)            \\
           \>    $x_{ij}$ \> $\le$ \> $\pi_j - \pi_i$ \> $\le$ \> $N x_{ij}$ \\
           \>             (an edge from i to j)            \\
           \>             \>       \> $\pi_i < \pi_j$ \>       \>            \\
           \>             (a \emph{fusion-preventing} edge from i to j)            \\
\\
           \> $x_{ij}$    \> $\le$ \> $c_i$           \>       \>            \\
           \> (for all edges from i with no fusion-preventing outputs)      \\
\\
           \> $x_{i'j}$   \> $\le$ \> $x_{ij}$        \>       \>            \\
           \> (if $\tau_i \not= \tau_j \wedge gen_i=\{i'\}$) \\
           \> $x_{ij'}$   \> $\le$ \> $x_{ij}$        \>       \>            \\
           \> (if $\tau_i \not= \tau_j \wedge gen_j=\{j'\}$) \\
\end{tabbing}


\subsubsection{The objective function}

To find the objective function, note that fusing loops can have three main benefits:
\begin{itemize}
\item
reducing memory traffic, such as multiple loops reading from the same array;
\item
removing intermediate arrays, thus reducing the amount of memory required;
\item
and reducing loop overhead, such as when two loops operate on different arrays of the same size.
\end{itemize}
However, these benefits cannot be considered in isolation; for example, fusing two loops to reduce loop overhead may remove potential fusion opportunities that reduce memory traffic.
When operating on large arrays that do not fit in cache, memory traffic dominates execution time.
An excessive number of intermediate arrays can also cause issues if all are live in memory at once, potentially leading to \emph{thrashing}.
The benefits of removing loop overhead are least of all; it should be performed if possible, but must never remove opportunities for other fusion.

This total ordering can be encoded into an ILP objective function as weights.
If the program graph contains $N$ combinators, then there are at most $N$ opportunities for fusion.
The encoding of loop overhead is weight $1$, removing intermediate arrays is weight $N$, and reducing memory traffic is weight $N^2$.
This ensures that no amount of loop overhead reduction can outweigh the benefit of removing an intermediate array,
and likewise no number of removed intermediate arrays can outweigh a reduction in memory traffic.
Although, it is worth noting that reducing memory traffic does \emph{tend} to remove intermediate arrays, and vice versa.

\begin{tabbing}
MMMMM   \= MMMM \= M \= \kill
Minimise   \>     \> $\Sigma_{(i,j) \in E} W_{ij} x_{ij}$   \\
           \> \> \> (memory traffic and loop overhead)         \\
           \> $+$ \> $\Sigma_{i \in V} N c_i$  \\
           \> \> \> (removing intermediate arrays)         \\
Subject to \> \ldots                                \\
Where      \>                                       \\
           \> $W_{ij} = N^2$ \> $~|$ \> $(i,j) \in E $         \\
           \> \> \> (fusing $i$ and $j$ will reduce memory traffic)         \\
           \> $W_{ij} = N^2$ \> $~|$ \> $\exists k. (k,i) \in E \wedge (k,j) \in E $     \\
           \> \> \> ($i$ and $j$ share an input array)                                         \\
           \> $W_{ij} = 1$   \> $~|$ \> $@otherwise@$                                                  \\
           \> \> \> (the only benefit is loop overhead)                                        \\
           \\
           \> $N = |V|$
\end{tabbing}



\subsection{Subtype optimisations}
For two clusters $i, j$, if $x_{ij} = 0 \vee \tau_i \not= \tau_j$, then $i$'s generator must be fused with $j$ and vice versa, recursively ($x_{ij} = 0 \implies check(i,j)$).
Problem: actually generating constraints for \emph{all} pairs of nodes with unequal types is excessive.
I see two solutions to this:
\begin{itemize}
\item
Generate fewer constraints and rely on transitivity.
This means that for each pair $i,j$ we wouldn't generate an $x_{ij}$ and thus not generate those constraints, unless $i$ or $j$ is a @filter@, or arc between $i$ and $j$.
This would help, but also means we can't \emph{minimise} on these removed $x_{ij}$, so the solution won't count the (rather smaller) benefits of fusing two non-connected nodes.
\item
Alternatively:
we can say, only generate constraints if there is no \emph{fusion preventing path} between $i$ and $j$.
Here, we generate fewer constraints than originally, but still more than above.
We also must preprocess the graph to find such blocking paths.
But we still get the minimisation to count non-connected nodes.
\end{itemize}

\begin{tabbing}
MMMMMM      \= MM   \=  MMMMMMMMMMMM    \=  \kill
$split$     \> @::@ \> $V \times E \to \{ \{name\} \}$      \\
MMMM        \= M    \= \kill
$split(vs,es)$ \\
    \> $=$  \> $\{clusterable(v,vs,es) | v \in vs\}$      \\
$clusterable(v,vs,es)$  \\
    \>$=$       \>$\{v' | v' \in vs$                                \\
    \>$\wedge$  \>$\forall p. p = path(v,v') \vee p = path(v',v)$   \\
    \>$\wedge$  \>$fusion~preventing \not\in p\}$\\
\end{tabbing}

\subsection{Better than stream fusion}
Abdallah suggests something like:
\[
\forall g.\ \exists c.\ ilp(g, c) \le_c streamfusion(g)
\]
ie, for any graph there exists a criterion to optimise against, where we are better than or equal to stream fusion.

\subsection{Cost function}
We argue that our cost function makes intuitive sense, by reducing the number of manifest arrays and loops.
However, due to the nature of our heuristic cost function we have no strong guarantees that the clustering with the lowest cost will actually be the clustering with the shortest runtime.
In general, since we do not know the expected sizes of the arrays, this is impossible to guarantee correctly.
It is possible that for certain restricted subsets of the language, a more sophisticated cost function may be able to guarantee optimality.
This is left to future work.

\subsection{Proof}
To prove correctness of our linear program formulation, we need to prove two different things.
Firstly, the formulation's constraints must always be satisfiable; that is, there must exist a variable assignment that satisfies all constraints.
This is rather simple to show, but guarantees that the linear program will always give an answer.
The next thing to show is that any produced clustering is legal: if a variable assignment satisfies the constraints, then it is a valid and legal clustering.
This means that, not only do we get \emph{an} answer, we also get the \emph{right} answer.

\subsubsection{Satisfiability}
For any program $p$, there exists a trivial clustering with no fusion at all.
We can use this as the variable assignment of $ilp(p)$.
For each pair of nodes $m,n \in p$, $x_{mn} = 1$ --- no fusion is possible.
For the $\pi$ variables, we must find a topographical ordering of the nodes in $p$, which is simple since we are assured it is a dag.

Now, prove that this assignment actually satisfies the constraints.


\subsubsection{Soundness}
For any program $p$ and variable asignment $v$, if $v$ satisfies the constraints for $ilp(p)$, the clustering denoted by $x_{ij}$ in $v$ is legal.

For a clustering to be legal, it must satisfy three constraints:
\begin{description}
\item[Acyclic]
after merging nodes of same cluster together, the resulting graph must be a dag
\item[Precedence preserving]
if there is an edge between two nodes $i$ and $j$, and they are not merged together, then we require $\pi_j > \pi_i$
\item[Fusion preventing]
likewise, if there is a fusion-preventing edge between two nodes $i$ and $j$, then we require $\pi_j > \pi_i$, which implies that they are not merged together
\item[Type constraint]
if two nodes $i$ and $j$ are in the same cluster, then $\tau_i = \tau_j$, or if $\tau_i$ is a subtype of $\tau_j$ (or $\tau_j$ is a subtype of $\tau_i$), then the \emph{generator} for $\tau_i$ (or $\tau_j$) must also be in the same cluster as $i$ and $j$.
    \\
    \TODO Actually, let us say $x_{ij} = 0 \implies check_{ij}$

\end{description}
where
\begin{tabbing}
MMMMM      \= M \= MMMMMMM \= MM \= \kill
$check$ \> @::@  \> $array \times array$ \> $\to$ \> $\mathbb{B}$ \\
MMMMM      \= M \= MMMMMM \= MM \= \kill
$check(i, j)$     \> $|$ \> $tau_i = tau_j$ \> $=$ \> $x_{i,j} = 0$                        \\
$check(i, j)$     \> $|$ \> $i' \in gen(i) $ \> $=$ \> $x_{i',j} = 0 \wedge x_{i,i'} = 0 \wedge check(i', j)$                        \\
$check(i, j)$     \> $|$ \> $j' \in gen(j) $ \> $=$ \> $x_{i,j'} = 0 \wedge x_{j,j'} = 0 \wedge check(i, j')$                        \\
$check(i, j)$     \> $|$ \> $tau_i \not= tau_j$\> $=$ \> $\bot$
\end{tabbing}






