%!TEX root = ../Main.tex
\section{Icicle Source}
\label{s:Source}

We present a cut down version of our implementation; our implementation has many more primitives, case expressions, as well as primitive contexts for windowing based on time, folding over groups, and so on.

The first example defines $\mi{sum}$ as a fold over the input data.
The initial value for the fold is $0$, followed by the previous value $s$ added to the current value $v$.

\begin{tabbing}
MM \= MM \= MMMMMMMMMMM \= \kill
$\mi{sum}~v$    \\
\> $=$  \> $@fold@~s~=~0~@then@~s~+~v$ \\
\> @in@ \> $s$ \\
\end{tabbing}

Next, we can define $\mi{count}$ and $\mi{mean}$ in terms of this sum.
\begin{tabbing}
MM \= MM \= MMMMMMMMMMM \= \kill
$\mi{count}$                                        \\
 \> $=$  \> $\mi{sum}~1$                            \\
                                                    \\
$\mi{mean}~v$                                       \\
 \> $=$  \> $\mi{sum}~v~/~\mi{count}$               \\
\end{tabbing}

We can use @filter@ to count only the days where the close price is higher than the open.
\begin{tabbing}
MM \= MM \= MMMMMMMMMMM \= \kill
$\mi{gap}$                                          \\
 \> $=$  \> $@filter@~\mi{close}~>~\mi{open}$       \\
 \> @in@ \> $\mi{count}$                            \\
                                                    \\
$\mi{proportion}$                                   \\
 \> $=$  \> $\mi{gap}~/~\mi{count}$
\end{tabbing}

Finally, we can use @group@ to put into buckets based on the percentage of growth.
Here the key for the map is the percentage of growth, and the value is the number of days with that amount of growth, divided by the total number of days.
\begin{tabbing}
MM \= MM \= MMMMMMMMMMM \= \kill
$\mi{growthPercent}$                                        \\
 \> $=$  \> $((\mi{close}-\mi{open}) \times 100 ) / \mi{open}$  \\
                                                            \\
$\mi{growthBy}$                                             \\
 \> $=$  \> $@let@~\mi{total} = \mi{count}$                 \\
 \> @in@ \> $@group@~\mi{growthPercent}$                    \\
 \> @in@ \> $\mi{count} / \mi{total}$                    \\
\end{tabbing}

\TODO{Mention resumables and bubblegum?}

\TODO{Talk about scan}

This language is not referentially transparent, which may seem odd, but this actually allows us to restrict the language and uphold our performance guarantees.
The input stream is implicitly passed as an argument to all parts of the program, but cannot be referred to explicitly.
Additionally, operations like @filter@ create a new context where the input stream is modified.
This means that a @let@ binding outside of a @filter@ can have different semantics when moved inside the @filter@, despite having the same body.
However, by passing this argument implicitly we remove a class of programs that would be hard to guarantee performance for.
By prohibiting the programmer from accessing the underlying stream, they cannot perform certain operations like zipping two streams with different filters together.

We formally define the grammar of the language in figure~\ref{fig:source:grammar}.
Note that general application is not allowed: arguments can only be applied to primitives and named functions.
Similarly, functions and primitives must be fully applied.
This, combined with the lack of lambda construction, means that higher order functions are outlawed.
While a first-order language (one lacking higher order functions) offers somewhat less abstraction, it simplifies the typing rules and makes it easier to support our performance guarantees.

While the examples shown allow user defined functions, there is no recursion allowed.
This means that all function definitions can be inlined into the user query with guaranteed termination.
When converting to the intermediate language, we assume that this inlining has occurred.

\subsection{Evaluation}

\input{figures/Source-Grammar.tex}

In figure~\ref{fig:source:eval} we describe the bigstep evaluation semantics of the language.
We use the shorthand $\ov{\Gamma}$ to mean a list of $\Gamma$, and this same overline syntax is used as a variable containing a list.
The judgment $\BigstepG{x}{\ov{v}}{v}$ uses a list $\ov{\Gamma}$, with one environment for each element of the input stream.
The environment $\Gamma$ is used for scalar variables that are not associated with any particular input element.
Next is the expression $x$ that is to be evaluated.
The last two are outputs $\ov{v}$ and $v$.
The output $\ov{v}$ is the intermediate result of the computation for each part of the input stream.
That is, each element of $\ov{v}$ is the result of evaluating $x$ on the input stream up to that element.
The final output, $v$, is the result at the end of the stream.
In most cases it is the same as the last element of $\ov{v}$, unless $\ov{v}$ is empty or for a variable lookup.

The reason for returning the result list $\ov{v}$ is to make implementing the @scan@ primitive easier.
This allows @scan@ itself to be evalauted effectively as a no-op, but does mean that each other rule has to implement its own @scan@ as well.


\TODO{It seems like we should be able to get rid of the single element return $v$, but then would it be strange with the return value being of length $max~1~|\ov{v}|$?}

When evaluating on an input such as our @stocks@ table, we would create an environment for each row, with open and close as the variables.
The rule (EvTop) simply uses the empty environment for the scalar environment, and ignores the stream output.

For variables, the rules (EvVar) and (EvVarInput) look up the variable in the contexts.
In the case of direct inputs as introduced by (EvTop), there is no corresponding entry in the scalar environment, so (EvVarInput) is used which returns an error as the scalar output.
In this way, if the input stream is used as an aggregate it will be an evaluation error, but if it is used as an element value and folded upon, the error value will be thrown away.
For normal variables, there should be a binding in all environments.

Let bindings are handled by rule (EvLet), which evaluates its definition, adds part of the stream result to each stream environments, and adds the scalar result to the scalar environment.

For the rule (EvFold), first the zero or initial value of the fold is computed, using only its scalar value.
The extra rules (ScanNil) and (ScanCons) are then used to repeatedly apply the fold computation to each stream input, starting from the initial value.
The list of fold values are added to the stream environments, and the last of the fold values, or the initial value if none, is added to the scalar environment.

In (EvFilter) the predicate is evaluated to a list of results. 
The stream environments are filtered according to this predicate list using $\mit{pack}$, and the rest of the expression is computed.
However, the result list of the rest of the expression is only the scan of the filtered stream inputs.
We use $\mit{extend}$ to add back parts of the stream that have been filtered out, but must first find the initial value of the scan to start from.

The primitive operator rules (EvAdd) and (EvGt) apply their operator to each element of the inputs.
The primitive (EvNat) returns a list of constant numbers, one for each stream element.
Finally, (EvScan) is a no-op since the hard work of computing the @scan@s is actually implemented in the rest of the rules.


\input{figures/Source-Eval.tex}
