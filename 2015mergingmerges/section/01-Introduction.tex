%!TEX root = ../Main.tex
\section{Introduction}
\label{s:Introduction}

Fusion for array programs is important for removing intermediate arrays, reducing memory traffic as well as allocations.
However, when dealing with data too large to fit in memory such as tables on disk, removing intermediate arrays becomes essential rather than just desirable.
Attempting to create an intermediate array of such amounts of data would lead to thrashing and swapping to disk, or perhaps even running out of swap.
For these situations, some sort of assurance of total fusion is required: either the program can be fused with no intermediate arrays or unbounded buffers, or it will not compile at all.

In addition, despite the years of work on array combinator fusion, scheduling dataflow networks and streaming languages, it seems one combinator has been cast aside as jetsam: the humble merge join.
Unlike the simple access patterns of append or zip, merge join consumes from each of its inputs in a value-dependent order, always pulling from the input with the lowest associated key.

Merge joins are the heart of database joins, where two tables are ordered by the same key.
They are the ``conquer'' part of the divide and conquer merge sort.
They can even be used to implement segmented appends.

% -- Use the space to argue why merges are needed, not the other way around.
% -- This material would be better in the conclusion.

For example, if we have two sets of values over time, one useful operation is a correlation: to find the intersection of the values at given times, and compute some new value based on the two values:
\begin{code}
correlate :: [(Date,Value)] -> [(Date,Value)]
          -> [(Date,Value)]
correlate c1 c2
 = let m1 = merge c1 c2
       g1 = group_by compute m1
   in  g1
\end{code}

The core of this algorithm is the @merge@ which intersperses the two inputs so equal dates are next to each other.
Now suppose we have a third set and wish to compute the correlation of the first and second, and the second and third.
Since we are already iterating over the second set, it would be ideal to compute this other correlation in the same iteration, fusing both correlations into a single loop.

\begin{code}
corr2     :: [(Date,Value)] -> [(Date,Value)]
          -> [(Date,Value)]
          ->([(Date,Value)],   [(Date,Value)])
corr2     c1 c2 c3
 = let m1 = merge c1 c2
       m2 = merge c2 c3
       g1 = group_by compute m1
       g2 = group_by compute m2
   in (g1, g2)
\end{code}

If this program were executed strictly, the first merge would be computed in total, reading @c1@ and @c2@, then the second merge would be computed, re-reading @c2@ from the start, and reading @c3@.
Laziness is no panacea either, as whether @c2@ may still be read if @g1@ were consumed in total, then @g2@ consumed.
Neither alone is sufficient to implement this as a single iteration.
Instead, the two merges are fused, and whenever either merge requires input from @c2@, we attempt to execute the other merge until it too attempts to read from @c2@.

We make the following contributions:
\begin{itemize}
\item a kind of deterministic finite automata, for expressing streaming computations with value-dependent rates (\S\ref{s:Machines});
\item some common combinators implemented with such machines (\S\ref{s:Machines:Combinators});
\item an algorithm for merging non-recursive pairs of these machines (\S\ref{s:Merging});
\item invariants required of such machines (\S\ref{s:Machines:Invariants});
\item and proofs of correctness (\S\ref{s:Proofs});
\end{itemize}


\newpage
