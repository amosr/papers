%!TEX root = ../Main.tex

\section{Evaluation}
\label{s:evaluation}

To evaluate Pipit, we have implemented the high-level logic of a time-triggered Controller Area Network (CAN) bus driver~\cite{ISO11898_4}.
The CAN bus is commonly found in safety-critical automotive and industrial settings.
The time-triggered network architecture defines a static schedule of network traffic.
All nodes on the network must adhere to the same schedule, which significantly increases the reliability of periodic messages~\cite{fuehrer2001time}.

At a high level, the schedule is described by a system matrix which consists of rows of basic cycles.
Each basic cycle consists of a sequence of actions to be performed at particular time-marks.
Actions in the schedule may not be relevant to all nodes, so each node has its own local array containing the relevant triggers; trigger actions include sending and receiving application-specific messages, sending reference messages, and triggering `watch' alerts.
The trigger action for receiving an application-specific message checks that a particular message has been received since the trigger was last executed; depending on this, the driver increments or decrements a message-status-counter, which will in turn signal an error once the upper limit is reached.
Reference messages start a new basic cycle and are used to synchronise the nodes.
Watch alerts are generally placed after the expected end of the cycle and are used to signal an error if no reference message is received.

The TTCAN protocol can be implemented in two levels of increasing complexity.
In the first level, reference messages contain the index of the newly-started cycle.
In the second level, the reference messages also contain the value of a global fractional clock and whether any gaps have occurred in the global clock, which allows other nodes to calibrate their own clocks.
We implement the first level as it is more amenable to software implementation~\cite{hartwich2002integration}.

The implementation defines a streaming function that takes a stream describing the current time, the state of the hardware, and any received messages.
It returns a stream of commands to be performed, such as sending a particular reference message.
The implementation defines a pure streaming function.
To actually interact with the hardware we assume a small hardware-interop layer that reads from the hardware registers and translates the commands to hardware-register writes, but we have not yet implemented this.
We package the driver's inputs into a record for convenience:

\begin{tabbing}
  MM \= bus_status: \= \kill
  @type@ driver_input = \{ \\
    \> local_time: \> network_time_unit; \\
    \> mode_cmd: \> option mode; \\
    \> tx_status: \> tx_status; \\
    \> bus_status: \> bus_status; \\
    \> rx_ref: \> option ref_message; \\
    \> rx_app: \> option app_message_index; \\
    \}
\end{tabbing}

Here, the local-time field denotes the time-since-boot in \emph{network time units}, which are based on the bitrate of the underlying network bus.
The mode-command is an optional field which indicates requests from the application to enter configuration or execution mode.
The transmission-status describes the status of the last transmission request and may be none, success, or various error conditions.
The bus-status describes whether the bus is currently idle, busy, or in an error state.
The two receive fields denote messages received from the bus; for application-specific messages the time-triggered logic only needs the message identifier.

The driver-logic returns a stream of commands for the hardware-interop layer to perform:

\begin{tabbing}
  MM \= enable_acks: \= \kill
  @type@ commands = \{ \\
  \> enable_acks: \> bool; \\
  \> tx_ref: \>       option ref_message; \\
  \> tx_app: \> option app_message_index; \\
  \> tx_delay: \>     network_time_unit; \\
 \}
\end{tabbing}

The enable-acknowledgements field denotes whether the hardware should respond to messages from other nodes with an acknowledgement bit; in the case of a severe error acknowledgements are disabled, as the node must not write to the bus at all.
The transmit fields denote whether to send a reference message or an application-specific message.
For application-specific messages, the hardware-interop layer maintains the transmission buffers containing the actual message payload.
To meet the schedule as closely as possible, the driver anticipates the next transmission and includes a transmission delay to tell the hardware exactly when to send the next message.

\subsection{Runtime}

The implementation includes an extension of the trigger-fetch logic described in \autoref{s:motivation}, as well as state machines for tracking node synchronisation, master status and fault handling.
We generate real-time C code as described in \autoref{s:extraction}.
We evaluated the generated C code by executing with randomised inputs and measuring the worst-case-execution-time on a Raspberry Pi Pico (RP2040) microcontroller.
The runtime of the driver logic is fairly stable: over 5,000 executions, the measured worst-case execution time was $114\mu{}s$, while the average was $107\mu{}s$ with a standard deviation of $2.3\mu{}s$.
Earlier work on fault-tolerant TTCAN~\cite{short2007fault} describes the required slot sizes --- the minimum time between triggers --- to achieve bus utilisation at different bus rates.
For a 125Kbit/s bus, a slot size of approximately 1,500$\mu{}s$ is required to achieve utilisation above 85 per cent.
For the maximum CAN bus rate of 1Mbit/s, the required slot size is $184\mu{}s$.
Further evaluation is required to ensure that the complete runtime including the hardware-interop layer is sufficient for full-speed CAN.

Our code generation can be improved in a few ways.
A common optimisation in Lustre is to fuse consecutive if-statements with the same condition~\cite{bourke2017formally}; such an optimisation seems useful here, as our treatment of optional values introduces repeated unpacking and repacking.
Some form of array fusion~\cite{robinson2017machine} may also be useful for removing redundant array operations.
Our current extraction generates a transition-system with a step function which returns a tuple of the updated state and result.
Composing these step functions together results in repeated boxing and unboxing of this tuple; we currently rely on the \fstar{} normaliser to remove this boxing.
In the future, we plan to build on the current proofs to implement a more-sophisticated encoding that introduces less overhead.

\subsection{Verification}

We have verified a simplified trigger-fetch mechanism, as presented earlier (\autoref{s:motivation}).
For comparison, we implemented the same logic in the Kind2 model-checker~\cite{champion2016kind2}.
The restrictions placed on the triggers array --- that triggers are sorted by time-mark, that there must be an adequate time-gap between a trigger and its next-enabled, and that a trigger's time-mark must be greater-than-or-equal-to its index --- are naturally expressed with quantifiers.
The Kind2 model-checker includes experimental array and quantifier support~\cite{kind2userdoc}.
% but uses a custom syntax for arrays with no compiler support.
Due to the experimental nature of these features, we had to work around some limitations: for example, the use of arrays and quantifiers disables IC3-based invariant generation; quantified variables cannot be used in function calls; and the use of top-level constant arrays caused runtime errors that rendered most properties invalid~\cite{kind2024toparray}.

We were able to verify the Kind2 implementation of the simplified trigger-fetch mechanism for trigger arrays containing up to 16 elements; above that, a 32-size array reported multiple runtime errors and did not terminate after several hours.
% Verification of a 4-size array took 11s of CPU time; an 8-size array took 13s; a 16-size array took 276s; and a 32-size array timed out after several hours and multiple runtime errors.
For reference, the M_TTCAN hardware implementation of TTCAN supports up to 64 triggers~\cite{bosch2019mttcan}.

We made a critical simplification in the Kind2 implementation, which was to modify the trigger-enabled set to be a single cycle index.
In the specification, the enabled set is implemented as a cycle-offset and repeat-factor.
Checking if a trigger is enabled in the current cycle requires nonlinear arithmetic, which is difficult for SMT solvers.
In our Pipit development, we can treat the definition of the cycle set abstractly.
However, in the Kind2 development, quantifiers cannot contain function calls, which means that we cannot hide the implementation of the enabled-set check by providing an abstract contract.
This limitation also makes the specification quite unwieldy, as functions must be manually inlined.

\autoref{f:evaluation:kind2-runtime} shows the verification runtime for different sizes of arrays; the Pipit version is parametric in the array size, and is thus verified for all sizes of arrays.
We ran these experiments on a 2020 M1 MacBook Air with 16 gigabytes of RAM.
Both Kind2 and Pipit developments of the simplified trigger-fetch logic are roughly the same size, on the order of two-hundred lines of code including comments.

\begin{figure}
  \center
\begin{tabular}{r|rr|rr|rr}
  & \multicolumn{4}{c|}{Kind2} & Pipit \\
  & \multicolumn{2}{c|}{simple enable-set} & \multicolumn{2}{c|}{full enable-set} & \\
  size & wall-clock & CPU & wall-clock & CPU & wall-clock & CPU \\
  \hline
  1 & 2s &  3s  & 6s & 12s & 6s & 6s \\
  2 & 3s &  3s  & 8s & 12s & 6s & 6s \\
  4 & 5s & 11s  & 12s & 24s & 6s & 6s \\
  8 & 8s & 13s  & 82s & 90s & 6s & 6s \\
  16 & 125s & 276s & \multicolumn{2}{c|}{error} & 6s & 6s \\
  32 & \multicolumn{2}{c|}{error} & \multicolumn{2}{c|}{error} & 6s & 6s \\
\end{tabular}
\caption{Verification time for trigger-fetch; simple enable-set uses a simplified version of the enable-set, while full enable-set uses bitwise arithmetic as in the TTCAN specification. The verification time for Pipit is a once-and-for-all proof that is parametric in the size of the array.}
\label{f:evaluation:kind2-runtime}
\end{figure}

We plan to verify the remainder of the TTCAN implementation and publish it separately.
Prior work formalising TTCAN has variously modeled the protocol itself~\cite{saha2007finite, pan2014modeling,li2018formal},
instances of the protocol~\cite{guo2020model},
and abstract models of TTCAN implementations~\cite{leen2006modeling}, but we are unaware of any prior work that has verified an \emph{executable} implementation of TTCAN.

Separately, Pipit has also been used to implement and verify a real-time controller for a coffee machine reservoir control system~\cite{robinson2023pipit}.
The reservoir has a float switch to sense the water level and a solenoid to allow the intake of water.
The specification includes a simple model of the water reservoir and shows that the reservoir does not exceed the maximum level under different failure-mode assumptions.
